{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "seed_value = 42   ## An arbitrary seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine dataset shape: (178, 13)\n",
      "wine target shape: (178,)\n",
      "Unique classes:  [0 1 2]\n",
      "Instances per class: [59 71 48]\n"
     ]
    }
   ],
   "source": [
    "wine = load_wine()        ## To make experiments on other datasetsi alter here !!!!\n",
    "\n",
    "print('Wine dataset shape:', wine.data.shape)\n",
    "print(\"wine target shape:\", wine.target.shape)\n",
    "print('Unique classes: ',np.unique(wine.target))  ##These unique classes stands for cultivars which the wine is obtained from.\n",
    "print(\"Instances per class:\", np.bincount(wine.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:   [[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "y:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "## Extracting features and labels\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "#  scaler = StandardScaler()\n",
    "#  X = scaler.fit_transform(X)\n",
    "\n",
    "## After scaling, the error rates went even lower than those in the original paper for some datasets.\n",
    "\n",
    "\n",
    "print(\"X:  \", X)\n",
    "print(\"y: \" , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_5_percent_samples(X, y, seed):\n",
    "\n",
    "    np.random.seed(seed)  ## Seed has been fixed to 42 in the beginning of the code.\n",
    "\n",
    "    # Determine the number of samples to select from each class (5%)\n",
    "    num_samples_per_class = {label: int(np.ceil(0.05 * np.sum(y == label))) for label in np.unique(y)}\n",
    "\n",
    "    ## For every unique class, I choose the (ceiling_%5) amount of instances. \n",
    "    ## I tried using floor function but the instances per class came out to be too low to get meaningful results.\n",
    "\n",
    "    # Initialize arrays to store the indices of labeled and unlabeled data\n",
    "    labeled_indices = np.array([], dtype=int)\n",
    "    unlabeled_indices = np.array([], dtype=int)\n",
    "\n",
    "    # Select 5% from each class as labeled data\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)[0] \n",
    "        \n",
    "## This will give me the indices of the instances which belong to the class 'label', which is a number from 0 to 2.\n",
    "        np.random.shuffle(indices)\n",
    "        label_indices = indices[:num_samples_per_class[label]]\n",
    "        unlabeled_indices = np.concatenate((unlabeled_indices, indices[num_samples_per_class[label]:]))\n",
    "        labeled_indices = np.concatenate((labeled_indices, label_indices))\n",
    "\n",
    "    # Split the data into labeled and unlabeled sets\n",
    "    X_labeled = X[labeled_indices]\n",
    "    y_labeled = y[labeled_indices]\n",
    "    \n",
    "    X_unlabeled = X[unlabeled_indices]\n",
    "    y_unlabeled = y[unlabeled_indices]\n",
    "    \n",
    "    return X_labeled, y_labeled, X_unlabeled, y_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments on   select_5_percent_samples(X, y, seed)\n",
    "    The function above selects 5% of the samples from each class as labeled data and the remaining as unlabeled data. \n",
    "    (As suggested in the paper for building the experiments)\n",
    "    This is because we are working on semi-supervised area, we need to convert our dataset accordingly\n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    - X: numpy array, input features.\n",
    "    - y: numpy array, input labels.\n",
    "    - seed: int, seed value for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    - X_labeled: numpy array, labeled data.\n",
    "    - y_labeled: numpy array, labels for the labeled data.\n",
    "    - X_unlabeled: numpy array, unlabeled data.\n",
    "    - y_unlabeled: numpy array, labels for the unlabeled data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def euclidean_distance_classifier(X_labeled, y_labeled, X_unlabeled, y_unlabeled, seed):\n",
    "    distances = cdist(X_unlabeled, X_labeled, metric='euclidean')\n",
    "    \n",
    "## = the Euc. dist between each instance in the unlabeled data and each instance in the labeled data.\n",
    "    \n",
    "    k = 1 # Assign labels based on majority vote of k-nearest neighbors (k=1)\n",
    "    y_pred_unlabeled_euc = []\n",
    "\n",
    "    for dist_row in distances:\n",
    "        nearest_indices = np.argsort(dist_row)[:k]\n",
    "        nearest_labels = y_labeled[nearest_indices]\n",
    "        majority_label = Counter(nearest_labels).most_common(1)[0][0]\n",
    "        y_pred_unlabeled_euc.append(majority_label)\n",
    "\n",
    "    error_rate_euc = 1 - accuracy_score(y_unlabeled, y_pred_unlabeled_euc)\n",
    "    ## Error rate is 1 - accuracy score of predicted unlabeled data.\n",
    "        \n",
    "    return error_rate_euc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments of euclidean_distance_classifier :\n",
    "\n",
    "    For every unlabeled instance, it finds the nearest labeled instance and assigns the label of the nearest labeled instance to that.\n",
    "\n",
    "     This process comes from the assumption of \"what is closest to you must be similar to you\".\n",
    "  \n",
    "     After the process, it computes the error rate of the Euclidean distance classifier on unlabeled data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_labeled: numpy array, labeled data.\n",
    "    - y_labeled: numpy array, labels for the labeled data.\n",
    "    - X_unlabeled: numpy array, unlabeled data.\n",
    "    - y_unlabeled: numpy array, labels for the unlabeled data.\n",
    "    - seed: int, seed value for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - error_rate_euc: float, error rate of the Euclidean distance classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric_learn import LMNN \n",
    "\n",
    "def apply_lmnn(X_labeled, y_labeled, K=3, max_iter=1000):\n",
    "    \n",
    "    ## After max_iter=1000, the error rate was very similar to max_iter=10000, so I chose 1000 to save time.\n",
    "    lmnn = LMNN(k=K, max_iter=max_iter)\n",
    "    lmnn.fit(X_labeled, y_labeled)\n",
    "\n",
    "    return lmnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments on apply_lmnn : \n",
    "\n",
    "    This function creates an instance lmnn from the LMNN class which is imported from the metric_learn library.\n",
    "    The fit method of the LMNN class trains the model using labeled datas.\n",
    "\n",
    "    To summerize, this applies the Large Margin Nearest Neighbor (LMNN) algorithm to learn a distance metric.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_labeled: numpy array, labeled data.\n",
    "    - y_labeled: numpy array, labels for the labeled data.\n",
    "    - K: int, number of nearest neighbors to consider.\n",
    "    - max_iter: int, maximum number of iterations for convergence.\n",
    "        -set to 1000 since it was in a good spot between time efficiency and convergence to limit value. \n",
    "    \n",
    "    Returns:\n",
    "    - lmnn: an LMNN object, trained LMNN model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def evaluate_lmnn(lmnn, X_labeled, y_labeled, X_unlabeled, y_unlabeled):\n",
    "\n",
    "    # Transform labeled data using the learned metric\n",
    "    X_labeled_transformed = lmnn.transform(X_labeled)\n",
    "    \n",
    "    # Train a k-nearest neighbors (k=1) classifier on transformed labeled data\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn_classifier.fit(X_labeled_transformed, y_labeled)\n",
    "\n",
    "    # Transform unlabeled data using the learned metric\n",
    "    X_unlabeled_transformed = lmnn.transform(X_unlabeled)\n",
    "    \n",
    "    # Predict labels for transformed unlabeled data\n",
    "    y_pred_unlabeled = knn_classifier.predict(X_unlabeled_transformed)\n",
    "    \n",
    "    # Calculate error rate on unlabeled data\n",
    "    unlabeled_error_rate = 1 - accuracy_score(y_unlabeled, y_pred_unlabeled)\n",
    "    \n",
    "    return unlabeled_error_rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Evaluate the error rate of LMNN on labeled and unlabeled data.\n",
    "    \n",
    "\n",
    "    The LMNN algorithm learns a transformation of the feature space that brings instances of the same class closer together while pushing instances of different classes fUrther apart. This applies the transformation to the labeled data, making it more suitable for classification with KNN.\n",
    "\n",
    "    Parameters:\n",
    "    - lmnn: LMNN object, trained LMNN model.\n",
    "    - X_labeled: numpy array, labeled data.\n",
    "    - y_labeled: numpy array, labels for the labeled data.\n",
    "    - X_unlabeled: numpy array, unlabeled data.\n",
    "    - y_unlabeled: numpy array, labels for the unlabeled data.\n",
    "    \n",
    "    Returns:\n",
    "    - unlabeled_error_rate: float, error rate of LMNN on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "## Metric_learn library used to print a future warning every single time for each experiment ...\n",
    "## ... so I had to add this for clearer looking output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##  Two compared methods' experiments are done. \n",
    "\n",
    "* ###  Now lets put S3ML (Semi Supervised Sparse Metric Learning) algorithm into code!\n",
    "\n",
    "\n",
    "\n",
    "##         S3ML:   (p.7)\n",
    "     Input: \n",
    "     Three sets X, S, D, an integer k, \n",
    "     four real-valued parameters 0 < α < 1, θ > 0, β > 0 and ρ > 0, \n",
    "     and input metric matrix M0.\n",
    "\n",
    "     Output: The sparse metric matrix M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: k-NN Search\n",
    "#### comments on construct_neighborhood_indicator_matrix:\n",
    "    \n",
    "    Construct the neighborhood indicator matrix P for labeled samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: numpy array, input features.\n",
    "    \n",
    "    Returns:\n",
    "    - P: numpy array, neighborhood indicator matrix.\n",
    "\n",
    "     the Output, P matrix, it looks like a transition probability matrix which is familiar to us from Markov chains.\n",
    "     Sum of all individual rows are always equal to 1. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 6 ## As sugested in the paper, page 7\n",
    "\n",
    "def construct_neighborhood_indicator_matrix(X):\n",
    "\n",
    "    # Initialize the nearest neighbors model and fit it to the labeled data\n",
    "    nn = NearestNeighbors(algorithm='auto', metric=\"euclidean\").fit(X)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X) ## For this matrix, we are not interested in distances, only indices.\n",
    "\n",
    "    # Initialize the neighborhood indicator matrix P with zeros\n",
    "    P = np.zeros((len(X), len(X)))\n",
    "\n",
    "    # Fill in the neighborhood indicator matrix P for the labeled samples\n",
    "    for i in range(len(X)):\n",
    "        P[i, indices[i]] = 1 / k\n",
    "    \n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [[0.16666667 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16666667 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.16666667 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.16666667 0.16666667 0.        ]\n",
      " [0.         0.         0.         ... 0.16666667 0.16666667 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Select 5% of the samples from each class as labeled\n",
    "X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "\n",
    "# Construct neighborhood indicator matrix P for the labeled data\n",
    "P = construct_neighborhood_indicator_matrix(X)\n",
    "\n",
    "print(\"P: \", P)  # Show the constructed matrix P for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Affinity Propogation \n",
    "\n",
    "#### comments on affinity_propogation:  \n",
    "     Interestingly, the proposed S3ML algorithm can also work under supervised settings when simply setting W = W0 \n",
    "     so S3ML is appropriate for various metric learning problems. (paper, p.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  ## Damping factor\n",
    "teta = 0.01  ## Treshold value to determine the strong affinities.\n",
    "## These two paramerer values are also mentioned in the paper, p.7 \n",
    "\n",
    "\n",
    "def affinity_propagation(P, y_labeled, alpha, teta):\n",
    "    # Initialize the affinity matrix W_0\n",
    "\n",
    "    n_samples = P.shape[0]\n",
    "    W_0 = np.zeros((n_samples, n_samples)) ## np.eye(n_samples) ## da olabilir. Tekrar iyi bak.\n",
    "    \n",
    "    ## Initially creates a matix full of zeros with the respective dimensions\n",
    "    \n",
    "    labeled_indices = np.where(y_labeled != -1)[0]\n",
    "\n",
    "    # Set affinities for similar and dissimilar pairs\n",
    "    for i, idx_i in enumerate(labeled_indices):\n",
    "        for j, idx_j in enumerate(labeled_indices):\n",
    "            if y_labeled[i] == y_labeled[j]: ## Initially we mark similar points as if in \n",
    "                W_0[idx_i, idx_j] = 1  ## Similar pairs, as written in the paper\n",
    "            else:\n",
    "                W_0[idx_i, idx_j] = -1  ## Dissimilar pairs\n",
    "    \n",
    "    ## W0 has been created using %5 labeled indices. \n",
    "    ## Now we need to propagate the affinities through the neighborhood structure.\n",
    "\n",
    "    # Propagate affinities through the neighborhood structure\n",
    "    W = np.zeros_like(W_0) ##Creates an empty matrix with the same dimensions as W_0.\n",
    "\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        W = (1 - alpha) * W_0 + alpha * np.dot(P, W)\n",
    "        \n",
    "        # Apply the threshold to determine strong affinities\n",
    "        W[np.abs(W) < teta] = 0\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments on def affinity_propagation:\n",
    "\n",
    "    Perform affinity propagation based on the neighborhood structure and labeled data.\n",
    "    \n",
    "    Parameters:\n",
    "    - P: numpy array, neighborhood indicator matrix.\n",
    "    - y_labeled: numpy array, labels for the labeled samples.\n",
    "    - alpha: float, damping factor for affinity propagation.\n",
    "    - threshold: float, threshold to determine strong affinities.\n",
    "    \n",
    "    Returns:\n",
    "    - W: numpy array, the final affinity matrix after propagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2(W,X):\n",
    "\n",
    "    W1 = np.sum(W, axis=1)  # This computes the sum of each row (axis=1 for rows)\n",
    "    D = np.diag(W1)  # This creates a diagonal matrix D from the vector W1\n",
    "\n",
    "    # Compute the graph Laplacian L\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the matrix T\n",
    "    T = X.T @ L @ X  ##  @ stands for matrix multiplication in numpy\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "## Now set, Σ = np.linalg.inv(M0) + beta * T  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For demonstration of W and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  [[0.55961542 0.55961542 0.55961542 ... 0.         0.         0.        ]\n",
      " [0.42764579 0.42764579 0.42764579 ... 0.         0.         0.        ]\n",
      " [0.56818182 0.56818182 0.56818182 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "T:  [[-9.54568428e+00 -6.08554032e+00  1.12907058e+01 -9.22562531e+01\n",
      "   6.81648480e+02 -1.03504556e+00  7.21936637e+00  2.76313960e-01\n",
      "   1.21207111e+01 -2.04612111e+01 -4.03095201e+00  4.08978234e+01\n",
      "  -5.11712340e+03]\n",
      " [ 2.21622812e+00 -8.23761702e+00  4.16420269e-01 -2.79527470e+01\n",
      "   8.79313623e+01  4.52387227e-01  1.45524621e+00 -1.51501332e-01\n",
      "   9.39567509e-01  2.95012153e+00 -2.66628867e-01  5.92544342e+00\n",
      "  -4.69749019e+02]\n",
      " [ 5.28707754e-01 -2.95295538e+00  1.51490911e-01 -2.98445964e+01\n",
      "   8.17996735e+01 -9.18279040e-01  9.85603175e-01 -2.07314867e-01\n",
      "   1.08347148e+00 -4.21660733e+00 -5.40177835e-01  7.37193717e+00\n",
      "  -1.23495294e+03]\n",
      " [ 3.51268447e-01 -2.75027106e+01 -1.48235292e+00 -3.20214139e+02\n",
      "   5.29169889e+02 -5.22951699e+00  1.15480163e+01 -1.41479980e+00\n",
      "   7.52176174e+00 -3.95461731e+01 -2.64657334e+00  5.62106971e+01\n",
      "  -7.43435808e+03]\n",
      " [ 1.58602152e+01 -8.25281813e+01  4.24257171e+01 -1.01514286e+03\n",
      "   1.38278378e+03 -1.84342761e+01  6.58539115e+01 -3.39343308e+00\n",
      "   7.24597659e+01 -1.22777821e+02 -2.66356143e+01  2.84223794e+02\n",
      "  -4.16801805e+04]\n",
      " [-1.71943772e+00 -2.14490181e+00  1.17833552e+00 -2.34224255e+01\n",
      "   1.20262953e+02 -4.22796688e+00 -1.64675866e+00 -4.37623541e-02\n",
      "   5.75615593e-01 -1.16305315e+01 -4.36914578e-01  8.84471143e+00\n",
      "  -2.14436875e+03]\n",
      " [-2.03398444e-01 -1.74028097e+00  1.88860900e+00 -1.71899212e+01\n",
      "   1.46881952e+02 -2.91516804e+00 -2.86602647e+00  1.41362348e-01\n",
      "  -1.03352879e+00 -1.02072398e+01 -7.94877130e-01  8.36129769e+00\n",
      "  -1.81113884e+03]\n",
      " [-4.68230438e-02 -4.47717467e-01 -2.77869418e-02 -3.51415987e+00\n",
      "   9.45920755e+00 -8.46920840e-02  2.37903704e-01 -8.12921475e-02\n",
      "   2.08149593e-01 -5.18974033e-02 -7.95488292e-02  1.14487898e+00\n",
      "  -1.24980765e+02]\n",
      " [ 1.24562824e+00 -2.43439216e+00  4.35338740e-01 -2.00662511e+01\n",
      "   7.80557520e+01 -1.63144290e+00 -2.19377157e+00 -2.23516668e-02\n",
      "  -6.15793501e+00 -6.33126705e+00 -5.02702418e-01  5.87458028e+00\n",
      "  -1.01156152e+03]\n",
      " [-7.15055770e+00 -9.20646854e-01  1.46586884e+00 -6.45451946e+01\n",
      "   2.41615935e+02 -1.04731296e+01 -5.36823564e+00  1.26769997e-01\n",
      "  -1.18865002e+00 -4.88062068e+01 -5.67821059e-01  1.92401329e+01\n",
      "  -6.10000350e+03]\n",
      " [-1.66463061e-03 -2.84992338e-01  8.80370176e-01 -6.09449283e+00\n",
      "   5.23331458e+01  3.16780383e-01  6.70444581e-01  1.01936374e-02\n",
      "   8.76889956e-01 -4.27581353e-01 -4.85458040e-01  3.18323864e+00\n",
      "  -3.14737723e+02]\n",
      " [ 1.36422885e+00 -5.84054882e-01  3.40012266e+00 -1.10099879e+01\n",
      "   1.36203969e+02  1.77165123e+00  2.16452609e+00  4.95526428e-01\n",
      "   3.31509338e+00  2.83107247e-01 -1.02373136e+00  4.12951664e+00\n",
      "  -3.57313201e+02]\n",
      " [-1.19960464e+03 -1.11851820e+03  1.53240500e+02 -1.16213411e+04\n",
      "   4.61253577e+04 -1.67891277e+03 -4.15712714e+02 -6.91628572e+01\n",
      "   4.58289396e+02 -5.57321791e+03 -2.15131691e+02  3.91054425e+03\n",
      "  -1.64629420e+06]]\n",
      "P shape:  (178, 178)\n",
      "W shape:  (178, 178)\n",
      "T shape:  (13, 13)\n"
     ]
    }
   ],
   "source": [
    "# Use the function to perform affinity propagation\n",
    "\n",
    "alpha = 0.5  # Example value for the damping factor\n",
    "threshold = 0.01  # Example threshold value for strong affinities\n",
    "\n",
    "W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "T = step_2(W,X) ## It is basically an (n * n) square matrix where n = #features of X\n",
    "\n",
    "\n",
    "## Verify the affinity matrix\n",
    "\n",
    "print(\"W: \",W)  \n",
    "print(\"T: \", T)\n",
    "\n",
    "print(\"P shape: \", P.shape)\n",
    "print(\"W shape: \", W.shape)\n",
    "print(\"T shape: \", T.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_sigma(M, rho, sigma):\n",
    "    # Apply Nesterov's smoothing technique\n",
    "    ## to smooth the l1 term\n",
    "    U_star = np.minimum(rho, np.maximum(M / sigma, -rho))\n",
    "    \n",
    "    return np.trace(U_star.T @ M) - (sigma / 2) * np.linalg.norm(U_star, 'fro')**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Comments on h_σ :\n",
    "    The smoothed h function using Nesterov's technique.\n",
    "    \n",
    "    Parameters:\n",
    "    - M: The matrix variable of the optimization problem.\n",
    "    - rho: The sparsity parameter.\n",
    "    - sigma: The smoothness parameter.\n",
    "    \n",
    "    Returns:\n",
    "    - The value of the smoothed h function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_h_sigma(M, rho, sigma):\n",
    "\n",
    "    return np.minimum(rho, np.maximum(M / sigma, -rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Comments on grad_hσ :\n",
    "    Gradient of the smoothed h function.\n",
    "    \n",
    "    Parameters:\n",
    "    - M: The matrix variable of the optimization problem.\n",
    "    - rho: The sparsity parameter.\n",
    "    - sigma: The smoothness parameter.\n",
    "    \n",
    "    Returns:\n",
    "    - The gradient of the smoothed h function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(M, Sigma):\n",
    "    fM = -np.log(np.linalg.det(M)) + np.trace(Sigma @ M)\n",
    "\n",
    "    return fM\n",
    "\n",
    "def grad_f(M,Σ):\n",
    "    grad_fM = -np.linalg.inv(M) + Σ\n",
    "\n",
    "    return grad_fM\n",
    "\n",
    "def Σ(X,y_labeled, alpha, beta):\n",
    "    M0 = np.eye(X.shape[1])  # Initialize M0 as the identity matrix\n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    T = step_2(W)\n",
    "    Σ = np.linalg.inv(M0) + beta * T  \n",
    "\n",
    "    return Σ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Since we defined the neccessary functions, we can move forward to Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALM(M_init, Y_init, mu, rho, sigma, Σ, max_iter=500):\n",
    "\n",
    "    M = M_init\n",
    "    Y = Y_init\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        # Step 1: Update M\n",
    "        grad_h_Y = grad_h_sigma(Y, rho, sigma)\n",
    "        M_next = M - mu * (grad_h_Y + grad_f(M,Σ))\n",
    "        \n",
    "        # Step 2: Update Y\n",
    "        grad_f_M_next = grad_f(M_next,Σ)\n",
    "        Y_next = Y - mu * (grad_f_M_next + grad_h_sigma(M_next, rho, sigma))\n",
    "        \n",
    "        \n",
    "        if np.linalg.norm(M_next - M, 'fro') < 1e-6:\n",
    "            break\n",
    "        \n",
    "        # Prepare for next iteration\n",
    "        M = M_next\n",
    "        Y = Y_next\n",
    "    \n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Iris, tuning parameter should be rho, beta = 100,   800\n",
    "### For wine ;                           rho, beta = 10000, 2 \n",
    "###### For wine ; rho, beta = 100000,11 === %10.7 (initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning parameters below (rho and beta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rho = 10000 ## Tuning (smoothness parameter)\n",
    "beta = 2   ## Tuning parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.000001  # Update step size\n",
    "sigma = 0.000001   # Smoothness parameter\n",
    "\n",
    "\n",
    "def run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta):\n",
    "\n",
    "    # Construct neighborhood indicator matrix P for the labeled data\n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    # Apply affinity propagation to obtain the affinity matrix W\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    # Compute matrix T\n",
    "    T = step_2(W,X)\n",
    "\n",
    "    # Set Σ = np.linalg.inv(M0) + beta * T  \n",
    "    Σ = np.linalg.inv(M0) + beta * T\n",
    "\n",
    "    # Run ALM to obtain the sparse metric matrix M\n",
    "    M = ALM( M0, Y0, mu, rho, sigma, Σ)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=1, metric=\"mahalanobis\" , metric_params={'VI': np.linalg.inv(M)})\n",
    "    knn.fit(X_labeled, y_labeled) ## Model training phase with the help of labeled data points.\n",
    "\n",
    "    # Predict labels for unlabeled data\n",
    "    y_pred = knn.predict(X_unlabeled)\n",
    "    # Calculate accuracy score and error rate\n",
    "    accuracy = accuracy_score(y_unlabeled, y_pred)\n",
    "    error_rate = 1 - accuracy\n",
    "    print(\"S3ML Error rate: \", error_rate)\n",
    "\n",
    "    return error_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_LMNN(experiment_number, LMNN_avg_error_rate, seed_value):\n",
    "\n",
    "    for _ in range(experiment_number):\n",
    "        X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "\n",
    "\n",
    "        # Train LMNN on labeled data\n",
    "        lmnn_model = apply_lmnn(X_labeled, y_labeled)\n",
    "\n",
    "        # Evaluate LMNN on both labeled and unlabeled data\n",
    "        unlabeled_error_rate = evaluate_lmnn(lmnn_model, X_labeled, y_labeled, X_unlabeled, y_unlabeled)\n",
    "\n",
    "        LMNN_avg_error_rate += unlabeled_error_rate\n",
    "        seed_value += 1 \n",
    "        \n",
    "        print(\"LMNN Error rate (trial \", _+1 , \"):\",  unlabeled_error_rate)\n",
    "        ## In every iteration we need to increase seed by one to maintain both randomness and stability.\n",
    "    print(\"Average error rate of LMNN method after \" , experiment_number ,\"tests: \", LMNN_avg_error_rate/experiment_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments on experiment_LMNN    \n",
    "    Perform experiments to evaluate the error rates of LMNN and Euclidean distance classifiers.\n",
    "    \n",
    "    Parameters:\n",
    "    - experiment_number: int, number of experiments to perform.\n",
    "    - LMNN_avg_error_rate: float, average error rate of LMNN.\n",
    "    - EU_avg_error_rate: float, average error rate of Euclidean distance classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_EU(experiment_number, EU_avg_error_rate, seed_value):\n",
    "\n",
    "    for _ in range(experiment_number):\n",
    "        X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "        error_rate_euc = euclidean_distance_classifier(X_labeled, y_labeled, X_unlabeled, y_unlabeled,seed_value)\n",
    "        EU_avg_error_rate += error_rate_euc\n",
    "        seed_value += 1\n",
    "        print(\"EU Error rate (trial\", _+1 , \"):\", error_rate_euc)\n",
    "\n",
    "\n",
    "    print(\"Average error rate of EU method after \" , experiment_number ,\"tests:   \", EU_avg_error_rate/experiment_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment is going to occur with those valuees accordingly.\n",
      " X labeled:  (10, 13) \n",
      " y labeled:  (10,) \n",
      " X unlabeled:  (168, 13) \n",
      " y unlaneled:  (168,) \n",
      "\n",
      "EXPERIMENT RESULTS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMNN Error rate (trial  1 ): 0.13095238095238093\n",
      "LMNN Error rate (trial  2 ): 0.26190476190476186\n",
      "LMNN Error rate (trial  3 ): 0.10119047619047616\n",
      "LMNN Error rate (trial  4 ): 0.20833333333333337\n",
      "LMNN Error rate (trial  5 ): 0.24404761904761907\n",
      "LMNN Error rate (trial  6 ): 0.1964285714285714\n",
      "LMNN Error rate (trial  7 ): 0.19047619047619047\n",
      "LMNN Error rate (trial  8 ): 0.10119047619047616\n",
      "LMNN Error rate (trial  9 ): 0.1964285714285714\n",
      "LMNN Error rate (trial  10 ): 0.32738095238095233\n",
      "LMNN Error rate (trial  11 ): 0.11309523809523814\n",
      "LMNN Error rate (trial  12 ): 0.0535714285714286\n",
      "LMNN Error rate (trial  13 ): 0.20833333333333337\n",
      "LMNN Error rate (trial  14 ): 0.32738095238095233\n",
      "LMNN Error rate (trial  15 ): 0.22619047619047616\n",
      "LMNN Error rate (trial  16 ): 0.18452380952380953\n",
      "LMNN Error rate (trial  17 ): 0.24404761904761907\n",
      "LMNN Error rate (trial  18 ): 0.2678571428571429\n",
      "LMNN Error rate (trial  19 ): 0.23809523809523814\n",
      "LMNN Error rate (trial  20 ): 0.20238095238095233\n",
      "LMNN Error rate (trial  21 ): 0.29761904761904767\n",
      "LMNN Error rate (trial  22 ): 0.24404761904761907\n",
      "LMNN Error rate (trial  23 ): 0.29166666666666663\n",
      "LMNN Error rate (trial  24 ): 0.13690476190476186\n",
      "LMNN Error rate (trial  25 ): 0.26190476190476186\n",
      "LMNN Error rate (trial  26 ): 0.1607142857142857\n",
      "LMNN Error rate (trial  27 ): 0.1428571428571429\n",
      "LMNN Error rate (trial  28 ): 0.1785714285714286\n",
      "LMNN Error rate (trial  29 ): 0.17261904761904767\n",
      "LMNN Error rate (trial  30 ): 0.15476190476190477\n",
      "LMNN Error rate (trial  31 ): 0.2142857142857143\n",
      "LMNN Error rate (trial  32 ): 0.2321428571428571\n",
      "LMNN Error rate (trial  33 ): 0.13095238095238093\n",
      "LMNN Error rate (trial  34 ): 0.29761904761904767\n",
      "LMNN Error rate (trial  35 ): 0.07738095238095233\n",
      "LMNN Error rate (trial  36 ): 0.1428571428571429\n",
      "LMNN Error rate (trial  37 ): 0.13095238095238093\n",
      "LMNN Error rate (trial  38 ): 0.18452380952380953\n",
      "LMNN Error rate (trial  39 ): 0.09523809523809523\n",
      "LMNN Error rate (trial  40 ): 0.27380952380952384\n",
      "LMNN Error rate (trial  41 ): 0.1071428571428571\n",
      "LMNN Error rate (trial  42 ): 0.13690476190476186\n",
      "LMNN Error rate (trial  43 ): 0.1428571428571429\n",
      "LMNN Error rate (trial  44 ): 0.22023809523809523\n",
      "LMNN Error rate (trial  45 ): 0.16666666666666663\n",
      "LMNN Error rate (trial  46 ): 0.2678571428571429\n",
      "LMNN Error rate (trial  47 ): 0.22023809523809523\n",
      "LMNN Error rate (trial  48 ): 0.2321428571428571\n",
      "LMNN Error rate (trial  49 ): 0.20238095238095233\n",
      "LMNN Error rate (trial  50 ): 0.19047619047619047\n",
      "Average error rate of LMNN method after  50 tests:  0.19464285714285703\n"
     ]
    }
   ],
   "source": [
    "experiment_number =50 ## In the paper, 50 experiments are made from every algorithm.\n",
    "\n",
    "X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "print(\"Experiment is going to occur with those valuees accordingly.\")\n",
    "print(\" X labeled: \", X_labeled.shape,\"\\n\", \"y labeled: \", y_labeled.shape,\"\\n\",\n",
    "       \"X unlabeled: \", X_unlabeled.shape, \"\\n\", \"y unlaneled: \",y_unlabeled.shape, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"EXPERIMENT RESULTS:\")\n",
    "experiment_LMNN(experiment_number, 0, seed_value) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU Error rate (trial 1 ): 0.3035714285714286\n",
      "EU Error rate (trial 2 ): 0.3392857142857143\n",
      "EU Error rate (trial 3 ): 0.3214285714285714\n",
      "EU Error rate (trial 4 ): 0.27380952380952384\n",
      "EU Error rate (trial 5 ): 0.32738095238095233\n",
      "EU Error rate (trial 6 ): 0.2857142857142857\n",
      "EU Error rate (trial 7 ): 0.34523809523809523\n",
      "EU Error rate (trial 8 ): 0.33333333333333337\n",
      "EU Error rate (trial 9 ): 0.38095238095238093\n",
      "EU Error rate (trial 10 ): 0.3035714285714286\n",
      "EU Error rate (trial 11 ): 0.38095238095238093\n",
      "EU Error rate (trial 12 ): 0.31547619047619047\n",
      "EU Error rate (trial 13 ): 0.2678571428571429\n",
      "EU Error rate (trial 14 ): 0.38690476190476186\n",
      "EU Error rate (trial 15 ): 0.2678571428571429\n",
      "EU Error rate (trial 16 ): 0.34523809523809523\n",
      "EU Error rate (trial 17 ): 0.35119047619047616\n",
      "EU Error rate (trial 18 ): 0.35119047619047616\n",
      "EU Error rate (trial 19 ): 0.29166666666666663\n",
      "EU Error rate (trial 20 ): 0.3035714285714286\n",
      "EU Error rate (trial 21 ): 0.29166666666666663\n",
      "EU Error rate (trial 22 ): 0.36309523809523814\n",
      "EU Error rate (trial 23 ): 0.32738095238095233\n",
      "EU Error rate (trial 24 ): 0.3214285714285714\n",
      "EU Error rate (trial 25 ): 0.43452380952380953\n",
      "EU Error rate (trial 26 ): 0.33333333333333337\n",
      "EU Error rate (trial 27 ): 0.29166666666666663\n",
      "EU Error rate (trial 28 ): 0.30952380952380953\n",
      "EU Error rate (trial 29 ): 0.27976190476190477\n",
      "EU Error rate (trial 30 ): 0.29166666666666663\n",
      "EU Error rate (trial 31 ): 0.29761904761904767\n",
      "EU Error rate (trial 32 ): 0.27976190476190477\n",
      "EU Error rate (trial 33 ): 0.375\n",
      "EU Error rate (trial 34 ): 0.41666666666666663\n",
      "EU Error rate (trial 35 ): 0.26190476190476186\n",
      "EU Error rate (trial 36 ): 0.3392857142857143\n",
      "EU Error rate (trial 37 ): 0.3214285714285714\n",
      "EU Error rate (trial 38 ): 0.5416666666666667\n",
      "EU Error rate (trial 39 ): 0.31547619047619047\n",
      "EU Error rate (trial 40 ): 0.2857142857142857\n",
      "EU Error rate (trial 41 ): 0.41666666666666663\n",
      "EU Error rate (trial 42 ): 0.31547619047619047\n",
      "EU Error rate (trial 43 ): 0.29166666666666663\n",
      "EU Error rate (trial 44 ): 0.30952380952380953\n",
      "EU Error rate (trial 45 ): 0.29761904761904767\n",
      "EU Error rate (trial 46 ): 0.2857142857142857\n",
      "EU Error rate (trial 47 ): 0.4107142857142857\n",
      "EU Error rate (trial 48 ): 0.31547619047619047\n",
      "EU Error rate (trial 49 ): 0.33333333333333337\n",
      "EU Error rate (trial 50 ): 0.24404761904761907\n",
      "Average error rate of EU method after  50 tests:    0.3275\n"
     ]
    }
   ],
   "source": [
    "experiment_EU(experiment_number, 0, seed_value + experiment_number) \n",
    "## I added the # experiments made so far to seed value to maintain randomness.\n",
    "\n",
    "\n",
    "## The results are very close to the results in the paper.\n",
    "## The last two steps takes 30 secs on average (for Wine dataset)\n",
    "##      \"\"         \"\"       2 secs on average (for Iris dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3ML Error rate:  0.07738095238095233\n",
      "S3ML Error rate:  0.13095238095238093\n",
      "S3ML Error rate:  0.1071428571428571\n",
      "S3ML Error rate:  0.1071428571428571\n",
      "S3ML Error rate:  0.24404761904761907\n",
      "S3ML Error rate:  0.11309523809523814\n",
      "S3ML Error rate:  0.1607142857142857\n",
      "S3ML Error rate:  0.07738095238095233\n",
      "S3ML Error rate:  0.1785714285714286\n",
      "S3ML Error rate:  0.15476190476190477\n",
      "S3ML Error rate:  0.11904761904761907\n",
      "S3ML Error rate:  0.0714285714285714\n",
      "S3ML Error rate:  0.13690476190476186\n",
      "S3ML Error rate:  0.1071428571428571\n",
      "S3ML Error rate:  0.11904761904761907\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.13095238095238093\n",
      "S3ML Error rate:  0.17261904761904767\n",
      "S3ML Error rate:  0.1785714285714286\n",
      "S3ML Error rate:  0.1964285714285714\n",
      "S3ML Error rate:  0.1785714285714286\n",
      "S3ML Error rate:  0.11309523809523814\n",
      "S3ML Error rate:  0.24404761904761907\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.1428571428571429\n",
      "S3ML Error rate:  0.20238095238095233\n",
      "S3ML Error rate:  0.11904761904761907\n",
      "S3ML Error rate:  0.13095238095238093\n",
      "S3ML Error rate:  0.11904761904761907\n",
      "S3ML Error rate:  0.1964285714285714\n",
      "S3ML Error rate:  0.15476190476190477\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.15476190476190477\n",
      "S3ML Error rate:  0.1428571428571429\n",
      "S3ML Error rate:  0.10119047619047616\n",
      "S3ML Error rate:  0.11309523809523814\n",
      "S3ML Error rate:  0.1428571428571429\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.15476190476190477\n",
      "S3ML Error rate:  0.1071428571428571\n",
      "S3ML Error rate:  0.1428571428571429\n",
      "S3ML Error rate:  0.10119047619047616\n",
      "S3ML Error rate:  0.18452380952380953\n",
      "S3ML Error rate:  0.13095238095238093\n",
      "S3ML Error rate:  0.125\n",
      "S3ML Error rate:  0.11309523809523814\n",
      "S3ML Error rate:  0.17261904761904767\n",
      "S3ML Error rate:  0.1607142857142857\n",
      "S3ML Error rate:  0.125\n",
      "Average error rate of S3ML after  50 tests:  0.13964285714285718\n"
     ]
    }
   ],
   "source": [
    "def experiment_S3ML(experiment_number,  seed_value,mu, rho, sigma, beta, avg_S3ML_error_rate):\n",
    "    for _ in range(experiment_number):\n",
    "        X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "        M0 = np.identity(X.shape[1])\n",
    "        Y0 = np.zeros_like(M0)\n",
    "\n",
    "        avg_S3ML_error_rate += run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "        seed_value += 1\n",
    "        \n",
    "    avg_S3ML_error_rate = avg_S3ML_error_rate/experiment_number\n",
    "\n",
    "    print(\"Average error rate of S3ML after \", experiment_number ,\"tests: \", avg_S3ML_error_rate)\n",
    "\n",
    "\n",
    "experiment_S3ML(50,  seed_value, mu, rho, sigma, beta,0)\n",
    "## 7 seconds on Wine and 0.5 seconds on Iris.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    The algorithm below is for searching optimal tuning parameters (rho and beta). \n",
    "####    I limited the domain to [1, 1000] for simplicity, the real domain is 1 to 1 million and step size multiplier is x1.2\n",
    "####    (That's why the error rate is higher than our findings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3ML Error rate:  0.27380952380952384\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "S3ML Error rate:  0.25595238095238093\n",
      "rho: 1, beta: 1, avg error rate: 0.29563492063492064\n",
      "S3ML Error rate:  0.27380952380952384\n",
      "S3ML Error rate:  0.3392857142857143\n",
      "S3ML Error rate:  0.20238095238095233\n",
      "rho: 2, beta: 1, avg error rate: 0.2718253968253968\n",
      "S3ML Error rate:  0.1964285714285714\n",
      "S3ML Error rate:  0.30952380952380953\n",
      "S3ML Error rate:  0.375\n",
      "rho: 4, beta: 1, avg error rate: 0.29365079365079366\n",
      "S3ML Error rate:  0.36904761904761907\n",
      "S3ML Error rate:  0.14880952380952384\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 8, beta: 1, avg error rate: 0.26785714285714285\n",
      "S3ML Error rate:  0.22023809523809523\n",
      "S3ML Error rate:  0.29166666666666663\n",
      "S3ML Error rate:  0.27380952380952384\n",
      "rho: 16, beta: 1, avg error rate: 0.2619047619047619\n",
      "S3ML Error rate:  0.22619047619047616\n",
      "S3ML Error rate:  0.25595238095238093\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 32, beta: 1, avg error rate: 0.25595238095238093\n",
      "S3ML Error rate:  0.25\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "rho: 64, beta: 1, avg error rate: 0.3134920634920635\n",
      "S3ML Error rate:  0.25595238095238093\n",
      "S3ML Error rate:  0.3214285714285714\n",
      "S3ML Error rate:  0.29166666666666663\n",
      "rho: 128, beta: 1, avg error rate: 0.28968253968253965\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "S3ML Error rate:  0.27380952380952384\n",
      "S3ML Error rate:  0.32738095238095233\n",
      "rho: 256, beta: 1, avg error rate: 0.29563492063492064\n",
      "S3ML Error rate:  0.20833333333333337\n",
      "S3ML Error rate:  0.26190476190476186\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 512, beta: 1, avg error rate: 0.251984126984127\n",
      "S3ML Error rate:  0.26190476190476186\n",
      "S3ML Error rate:  0.22619047619047616\n",
      "S3ML Error rate:  0.22619047619047616\n",
      "rho: 1024, beta: 1, avg error rate: 0.23809523809523805\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "S3ML Error rate:  0.47023809523809523\n",
      "S3ML Error rate:  0.3928571428571429\n",
      "rho: 1, beta: 2, avg error rate: 0.4226190476190476\n",
      "S3ML Error rate:  0.6428571428571428\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "S3ML Error rate:  0.45833333333333337\n",
      "rho: 2, beta: 2, avg error rate: 0.5218253968253969\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "S3ML Error rate:  0.5119047619047619\n",
      "S3ML Error rate:  0.31547619047619047\n",
      "rho: 4, beta: 2, avg error rate: 0.39484126984126977\n",
      "S3ML Error rate:  0.5\n",
      "S3ML Error rate:  0.35119047619047616\n",
      "S3ML Error rate:  0.45238095238095233\n",
      "rho: 8, beta: 2, avg error rate: 0.4345238095238095\n",
      "S3ML Error rate:  0.26190476190476186\n",
      "S3ML Error rate:  0.30952380952380953\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 16, beta: 2, avg error rate: 0.2857142857142857\n",
      "S3ML Error rate:  0.42261904761904767\n",
      "S3ML Error rate:  0.375\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "rho: 32, beta: 2, avg error rate: 0.42063492063492064\n",
      "S3ML Error rate:  0.25595238095238093\n",
      "S3ML Error rate:  0.29761904761904767\n",
      "S3ML Error rate:  0.41666666666666663\n",
      "rho: 64, beta: 2, avg error rate: 0.32341269841269843\n",
      "S3ML Error rate:  0.22619047619047616\n",
      "S3ML Error rate:  0.32738095238095233\n",
      "S3ML Error rate:  0.3392857142857143\n",
      "rho: 128, beta: 2, avg error rate: 0.2976190476190476\n",
      "S3ML Error rate:  0.44047619047619047\n",
      "S3ML Error rate:  0.33333333333333337\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "rho: 256, beta: 2, avg error rate: 0.376984126984127\n",
      "S3ML Error rate:  0.375\n",
      "S3ML Error rate:  0.36309523809523814\n",
      "S3ML Error rate:  0.29761904761904767\n",
      "rho: 512, beta: 2, avg error rate: 0.3452380952380953\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "S3ML Error rate:  0.375\n",
      "rho: 1024, beta: 2, avg error rate: 0.3670634920634921\n",
      "S3ML Error rate:  0.5297619047619048\n",
      "S3ML Error rate:  0.35119047619047616\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "rho: 1, beta: 4, avg error rate: 0.4920634920634921\n",
      "S3ML Error rate:  0.44047619047619047\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "rho: 2, beta: 4, avg error rate: 0.5297619047619048\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "rho: 4, beta: 4, avg error rate: 0.4603174603174603\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.5892857142857143\n",
      "S3ML Error rate:  0.5476190476190477\n",
      "rho: 8, beta: 4, avg error rate: 0.5833333333333334\n",
      "S3ML Error rate:  0.6190476190476191\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.7261904761904762\n",
      "rho: 16, beta: 4, avg error rate: 0.6349206349206349\n",
      "S3ML Error rate:  0.7619047619047619\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "S3ML Error rate:  0.47023809523809523\n",
      "rho: 32, beta: 4, avg error rate: 0.5734126984126985\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.5476190476190477\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "rho: 64, beta: 4, avg error rate: 0.5396825396825397\n",
      "S3ML Error rate:  0.2142857142857143\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "rho: 128, beta: 4, avg error rate: 0.41071428571428575\n",
      "S3ML Error rate:  0.47619047619047616\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.45833333333333337\n",
      "rho: 256, beta: 4, avg error rate: 0.5119047619047619\n",
      "S3ML Error rate:  0.44047619047619047\n",
      "S3ML Error rate:  0.375\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "rho: 512, beta: 4, avg error rate: 0.45634920634920634\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "rho: 1024, beta: 4, avg error rate: 0.4900793650793651\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "rho: 1, beta: 8, avg error rate: 0.5416666666666666\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "rho: 2, beta: 8, avg error rate: 0.5059523809523809\n",
      "S3ML Error rate:  0.39880952380952384\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "rho: 4, beta: 8, avg error rate: 0.3968253968253968\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "rho: 8, beta: 8, avg error rate: 0.4305555555555555\n",
      "S3ML Error rate:  0.5476190476190477\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "S3ML Error rate:  0.5416666666666667\n",
      "rho: 16, beta: 8, avg error rate: 0.511904761904762\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.3214285714285714\n",
      "S3ML Error rate:  0.42261904761904767\n",
      "rho: 32, beta: 8, avg error rate: 0.44841269841269843\n",
      "S3ML Error rate:  0.5714285714285714\n",
      "S3ML Error rate:  0.6488095238095238\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "rho: 64, beta: 8, avg error rate: 0.5932539682539683\n",
      "S3ML Error rate:  0.5714285714285714\n",
      "S3ML Error rate:  0.6369047619047619\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "rho: 128, beta: 8, avg error rate: 0.5634920634920634\n",
      "S3ML Error rate:  0.6071428571428572\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.7261904761904762\n",
      "rho: 256, beta: 8, avg error rate: 0.6309523809523809\n",
      "S3ML Error rate:  0.36309523809523814\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "rho: 512, beta: 8, avg error rate: 0.4166666666666667\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "S3ML Error rate:  0.45833333333333337\n",
      "S3ML Error rate:  0.36904761904761907\n",
      "rho: 1024, beta: 8, avg error rate: 0.47420634920634924\n",
      "S3ML Error rate:  0.45833333333333337\n",
      "S3ML Error rate:  0.6964285714285714\n",
      "S3ML Error rate:  0.5476190476190477\n",
      "rho: 1, beta: 16, avg error rate: 0.5674603174603174\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.5892857142857143\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "rho: 2, beta: 16, avg error rate: 0.5376984126984127\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "S3ML Error rate:  0.47023809523809523\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "rho: 4, beta: 16, avg error rate: 0.48809523809523814\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "rho: 8, beta: 16, avg error rate: 0.501984126984127\n",
      "S3ML Error rate:  0.4285714285714286\n",
      "S3ML Error rate:  0.6071428571428572\n",
      "S3ML Error rate:  0.49404761904761907\n",
      "rho: 16, beta: 16, avg error rate: 0.509920634920635\n",
      "S3ML Error rate:  0.44047619047619047\n",
      "S3ML Error rate:  0.41666666666666663\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "rho: 32, beta: 16, avg error rate: 0.44047619047619047\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.41666666666666663\n",
      "S3ML Error rate:  0.625\n",
      "rho: 64, beta: 16, avg error rate: 0.5198412698412698\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "S3ML Error rate:  0.49404761904761907\n",
      "S3ML Error rate:  0.5119047619047619\n",
      "rho: 128, beta: 16, avg error rate: 0.5337301587301587\n",
      "S3ML Error rate:  0.47619047619047616\n",
      "S3ML Error rate:  0.35119047619047616\n",
      "S3ML Error rate:  0.5119047619047619\n",
      "rho: 256, beta: 16, avg error rate: 0.4464285714285714\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "S3ML Error rate:  0.3392857142857143\n",
      "S3ML Error rate:  0.30952380952380953\n",
      "rho: 512, beta: 16, avg error rate: 0.40079365079365087\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "S3ML Error rate:  0.2321428571428571\n",
      "rho: 1024, beta: 16, avg error rate: 0.37103174603174605\n",
      "S3ML Error rate:  0.8571428571428572\n",
      "S3ML Error rate:  0.8571428571428572\n",
      "S3ML Error rate:  0.7261904761904762\n",
      "rho: 1, beta: 32, avg error rate: 0.8134920634920636\n",
      "S3ML Error rate:  0.875\n",
      "S3ML Error rate:  0.7976190476190477\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "rho: 2, beta: 32, avg error rate: 0.7579365079365079\n",
      "S3ML Error rate:  0.875\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.8392857142857143\n",
      "rho: 4, beta: 32, avg error rate: 0.7718253968253969\n",
      "S3ML Error rate:  0.8333333333333334\n",
      "S3ML Error rate:  0.8630952380952381\n",
      "S3ML Error rate:  0.7083333333333333\n",
      "rho: 8, beta: 32, avg error rate: 0.8015873015873017\n",
      "S3ML Error rate:  0.8571428571428572\n",
      "S3ML Error rate:  0.7738095238095238\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "rho: 16, beta: 32, avg error rate: 0.7440476190476191\n",
      "S3ML Error rate:  0.8333333333333334\n",
      "S3ML Error rate:  0.8452380952380952\n",
      "S3ML Error rate:  0.7857142857142857\n",
      "rho: 32, beta: 32, avg error rate: 0.8214285714285715\n",
      "S3ML Error rate:  0.8392857142857143\n",
      "S3ML Error rate:  0.8273809523809523\n",
      "S3ML Error rate:  0.8452380952380952\n",
      "rho: 64, beta: 32, avg error rate: 0.8373015873015873\n",
      "S3ML Error rate:  0.8869047619047619\n",
      "S3ML Error rate:  0.6190476190476191\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "rho: 128, beta: 32, avg error rate: 0.7003968253968255\n",
      "S3ML Error rate:  0.4642857142857143\n",
      "S3ML Error rate:  0.5\n",
      "S3ML Error rate:  0.5833333333333333\n",
      "rho: 256, beta: 32, avg error rate: 0.5158730158730158\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "S3ML Error rate:  0.5\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "rho: 512, beta: 32, avg error rate: 0.48809523809523814\n",
      "S3ML Error rate:  0.8690476190476191\n",
      "S3ML Error rate:  0.6845238095238095\n",
      "S3ML Error rate:  0.7142857142857143\n",
      "rho: 1024, beta: 32, avg error rate: 0.7559523809523809\n",
      "S3ML Error rate:  0.5952380952380952\n",
      "S3ML Error rate:  0.8333333333333334\n",
      "S3ML Error rate:  0.5892857142857143\n",
      "rho: 1, beta: 64, avg error rate: 0.6726190476190476\n",
      "S3ML Error rate:  0.6666666666666667\n",
      "S3ML Error rate:  0.7619047619047619\n",
      "S3ML Error rate:  0.7440476190476191\n",
      "rho: 2, beta: 64, avg error rate: 0.7242063492063492\n",
      "S3ML Error rate:  0.875\n",
      "S3ML Error rate:  0.7440476190476191\n",
      "S3ML Error rate:  0.7916666666666666\n",
      "rho: 4, beta: 64, avg error rate: 0.8035714285714285\n",
      "S3ML Error rate:  0.75\n",
      "S3ML Error rate:  0.5773809523809523\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 8, beta: 64, avg error rate: 0.5376984126984127\n",
      "S3ML Error rate:  0.8511904761904762\n",
      "S3ML Error rate:  0.8690476190476191\n",
      "S3ML Error rate:  0.6547619047619048\n",
      "rho: 16, beta: 64, avg error rate: 0.7916666666666666\n",
      "S3ML Error rate:  0.5\n",
      "S3ML Error rate:  0.7976190476190477\n",
      "S3ML Error rate:  0.7142857142857143\n",
      "rho: 32, beta: 64, avg error rate: 0.6706349206349206\n",
      "S3ML Error rate:  0.7142857142857143\n",
      "S3ML Error rate:  0.6785714285714286\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "rho: 64, beta: 64, avg error rate: 0.6369047619047619\n",
      "S3ML Error rate:  0.6666666666666667\n",
      "S3ML Error rate:  0.7321428571428572\n",
      "S3ML Error rate:  0.7261904761904762\n",
      "rho: 128, beta: 64, avg error rate: 0.7083333333333334\n",
      "S3ML Error rate:  0.7083333333333333\n",
      "S3ML Error rate:  0.8690476190476191\n",
      "S3ML Error rate:  0.6845238095238095\n",
      "rho: 256, beta: 64, avg error rate: 0.753968253968254\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.7261904761904762\n",
      "rho: 512, beta: 64, avg error rate: 0.6190476190476191\n",
      "S3ML Error rate:  0.9226190476190477\n",
      "S3ML Error rate:  0.8392857142857143\n",
      "S3ML Error rate:  0.8511904761904762\n",
      "rho: 1024, beta: 64, avg error rate: 0.871031746031746\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "S3ML Error rate:  0.8214285714285714\n",
      "S3ML Error rate:  0.5833333333333333\n",
      "rho: 1, beta: 128, avg error rate: 0.6468253968253969\n",
      "S3ML Error rate:  0.625\n",
      "S3ML Error rate:  0.5714285714285714\n",
      "S3ML Error rate:  0.6964285714285714\n",
      "rho: 2, beta: 128, avg error rate: 0.6309523809523809\n",
      "S3ML Error rate:  0.5476190476190477\n",
      "S3ML Error rate:  0.7857142857142857\n",
      "S3ML Error rate:  0.7142857142857143\n",
      "rho: 4, beta: 128, avg error rate: 0.6825396825396827\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.5297619047619048\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "rho: 8, beta: 128, avg error rate: 0.5317460317460317\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "S3ML Error rate:  0.7321428571428572\n",
      "S3ML Error rate:  0.7440476190476191\n",
      "rho: 16, beta: 128, avg error rate: 0.6527777777777778\n",
      "S3ML Error rate:  0.7619047619047619\n",
      "S3ML Error rate:  0.7321428571428572\n",
      "S3ML Error rate:  0.7916666666666666\n",
      "rho: 32, beta: 128, avg error rate: 0.7619047619047619\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "S3ML Error rate:  0.8035714285714286\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "rho: 64, beta: 128, avg error rate: 0.6071428571428572\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "S3ML Error rate:  0.5773809523809523\n",
      "rho: 128, beta: 128, avg error rate: 0.5634920634920635\n",
      "S3ML Error rate:  0.29166666666666663\n",
      "S3ML Error rate:  0.47619047619047616\n",
      "S3ML Error rate:  0.47023809523809523\n",
      "rho: 256, beta: 128, avg error rate: 0.41269841269841273\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.5178571428571428\n",
      "rho: 512, beta: 128, avg error rate: 0.5317460317460317\n",
      "S3ML Error rate:  0.6488095238095238\n",
      "S3ML Error rate:  0.6547619047619048\n",
      "S3ML Error rate:  0.7380952380952381\n",
      "rho: 1024, beta: 128, avg error rate: 0.6805555555555557\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.2321428571428571\n",
      "S3ML Error rate:  0.6369047619047619\n",
      "rho: 1, beta: 256, avg error rate: 0.49007936507936506\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.7440476190476191\n",
      "S3ML Error rate:  0.7202380952380952\n",
      "rho: 2, beta: 256, avg error rate: 0.6746031746031745\n",
      "S3ML Error rate:  0.8392857142857143\n",
      "S3ML Error rate:  0.5238095238095238\n",
      "S3ML Error rate:  0.43452380952380953\n",
      "rho: 4, beta: 256, avg error rate: 0.5992063492063492\n",
      "S3ML Error rate:  0.3214285714285714\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.6964285714285714\n",
      "rho: 8, beta: 256, avg error rate: 0.5436507936507936\n",
      "S3ML Error rate:  0.43452380952380953\n",
      "S3ML Error rate:  0.5416666666666667\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "rho: 16, beta: 256, avg error rate: 0.48611111111111116\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "S3ML Error rate:  0.6011904761904762\n",
      "S3ML Error rate:  0.42261904761904767\n",
      "rho: 32, beta: 256, avg error rate: 0.5039682539682541\n",
      "S3ML Error rate:  0.5119047619047619\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "S3ML Error rate:  0.5416666666666667\n",
      "rho: 64, beta: 256, avg error rate: 0.4781746031746032\n",
      "S3ML Error rate:  0.375\n",
      "S3ML Error rate:  0.36904761904761907\n",
      "S3ML Error rate:  0.6547619047619048\n",
      "rho: 128, beta: 256, avg error rate: 0.4662698412698412\n",
      "S3ML Error rate:  0.5595238095238095\n",
      "S3ML Error rate:  0.6964285714285714\n",
      "S3ML Error rate:  0.6726190476190477\n",
      "rho: 256, beta: 256, avg error rate: 0.6428571428571429\n",
      "S3ML Error rate:  0.6190476190476191\n",
      "S3ML Error rate:  0.32738095238095233\n",
      "S3ML Error rate:  0.38095238095238093\n",
      "rho: 512, beta: 256, avg error rate: 0.44246031746031744\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "S3ML Error rate:  0.75\n",
      "S3ML Error rate:  0.5654761904761905\n",
      "rho: 1024, beta: 256, avg error rate: 0.5992063492063492\n",
      "S3ML Error rate:  0.47023809523809523\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "rho: 1, beta: 512, avg error rate: 0.4246031746031746\n",
      "S3ML Error rate:  0.45833333333333337\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.3035714285714286\n",
      "rho: 2, beta: 512, avg error rate: 0.4583333333333334\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "S3ML Error rate:  0.5416666666666667\n",
      "S3ML Error rate:  0.2857142857142857\n",
      "rho: 4, beta: 512, avg error rate: 0.4246031746031746\n",
      "S3ML Error rate:  0.39880952380952384\n",
      "S3ML Error rate:  0.5059523809523809\n",
      "S3ML Error rate:  0.35119047619047616\n",
      "rho: 8, beta: 512, avg error rate: 0.41865079365079366\n",
      "S3ML Error rate:  0.43452380952380953\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "S3ML Error rate:  0.45238095238095233\n",
      "rho: 16, beta: 512, avg error rate: 0.4305555555555555\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "S3ML Error rate:  0.3392857142857143\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "rho: 32, beta: 512, avg error rate: 0.41071428571428575\n",
      "S3ML Error rate:  0.44047619047619047\n",
      "S3ML Error rate:  0.5833333333333333\n",
      "S3ML Error rate:  0.48809523809523814\n",
      "rho: 64, beta: 512, avg error rate: 0.503968253968254\n",
      "S3ML Error rate:  0.1964285714285714\n",
      "S3ML Error rate:  0.7321428571428572\n",
      "S3ML Error rate:  0.5654761904761905\n",
      "rho: 128, beta: 512, avg error rate: 0.498015873015873\n",
      "S3ML Error rate:  0.375\n",
      "S3ML Error rate:  0.45238095238095233\n",
      "S3ML Error rate:  0.5357142857142857\n",
      "rho: 256, beta: 512, avg error rate: 0.45436507936507936\n",
      "S3ML Error rate:  0.6488095238095238\n",
      "S3ML Error rate:  0.6190476190476191\n",
      "S3ML Error rate:  0.49404761904761907\n",
      "rho: 512, beta: 512, avg error rate: 0.5873015873015873\n",
      "S3ML Error rate:  0.32738095238095233\n",
      "S3ML Error rate:  0.29761904761904767\n",
      "S3ML Error rate:  0.4285714285714286\n",
      "rho: 1024, beta: 512, avg error rate: 0.3511904761904762\n",
      "S3ML Error rate:  0.5654761904761905\n",
      "S3ML Error rate:  0.4464285714285714\n",
      "S3ML Error rate:  0.49404761904761907\n",
      "rho: 1, beta: 1024, avg error rate: 0.501984126984127\n",
      "S3ML Error rate:  0.34523809523809523\n",
      "S3ML Error rate:  0.5892857142857143\n",
      "S3ML Error rate:  0.38690476190476186\n",
      "rho: 2, beta: 1024, avg error rate: 0.44047619047619047\n",
      "S3ML Error rate:  0.4821428571428571\n",
      "S3ML Error rate:  0.5535714285714286\n",
      "S3ML Error rate:  0.27380952380952384\n",
      "rho: 4, beta: 1024, avg error rate: 0.43650793650793646\n",
      "S3ML Error rate:  0.39880952380952384\n",
      "S3ML Error rate:  0.4285714285714286\n",
      "S3ML Error rate:  0.29166666666666663\n",
      "rho: 8, beta: 1024, avg error rate: 0.373015873015873\n",
      "S3ML Error rate:  0.35119047619047616\n",
      "S3ML Error rate:  0.3571428571428571\n",
      "S3ML Error rate:  0.40476190476190477\n",
      "rho: 16, beta: 1024, avg error rate: 0.37103174603174605\n",
      "S3ML Error rate:  0.6130952380952381\n",
      "S3ML Error rate:  0.29761904761904767\n",
      "S3ML Error rate:  0.36904761904761907\n",
      "rho: 32, beta: 1024, avg error rate: 0.4265873015873016\n",
      "S3ML Error rate:  0.45238095238095233\n",
      "S3ML Error rate:  0.5833333333333333\n",
      "S3ML Error rate:  0.32738095238095233\n",
      "rho: 64, beta: 1024, avg error rate: 0.4543650793650793\n",
      "S3ML Error rate:  0.3035714285714286\n",
      "S3ML Error rate:  0.33333333333333337\n",
      "S3ML Error rate:  0.47619047619047616\n",
      "rho: 128, beta: 1024, avg error rate: 0.37103174603174605\n",
      "S3ML Error rate:  0.47619047619047616\n",
      "S3ML Error rate:  0.5297619047619048\n",
      "S3ML Error rate:  0.26190476190476186\n",
      "rho: 256, beta: 1024, avg error rate: 0.4226190476190476\n",
      "S3ML Error rate:  0.5892857142857143\n",
      "S3ML Error rate:  0.38690476190476186\n",
      "S3ML Error rate:  0.27380952380952384\n",
      "rho: 512, beta: 1024, avg error rate: 0.4166666666666667\n",
      "Best rho: 1024, Best beta: 1, Best error rate: 0.23809523809523805\n"
     ]
    }
   ],
   "source": [
    "###     Grid search (extra):\n",
    "\n",
    "def search_optimal_parameters(X, y, seed_value, mu, sigma, start_value=1, max_value=1000, experiment_number=3):\n",
    "    best_rho = start_value\n",
    "    best_beta = start_value\n",
    "    best_error_rate = float('inf')\n",
    "\n",
    "    rho = start_value\n",
    "    beta = start_value\n",
    "\n",
    "    while rho <= max_value or beta <= max_value:\n",
    "        avg_error_rate = 0\n",
    "        for _ in range(experiment_number):\n",
    "            X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "            M0 = np.identity(X.shape[1])\n",
    "            Y0 = np.zeros_like(M0)\n",
    "\n",
    "            error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "            seed_value += 1\n",
    "            avg_error_rate += error_rate\n",
    "\n",
    "        avg_error_rate /= experiment_number\n",
    "\n",
    "        print(f\"rho: {rho}, beta: {beta}, avg error rate: {avg_error_rate}\")\n",
    "\n",
    "        if avg_error_rate < best_error_rate:\n",
    "            best_error_rate = avg_error_rate\n",
    "            best_rho = rho\n",
    "            best_beta = beta\n",
    "\n",
    "        if rho <= max_value:\n",
    "            rho *= 2\n",
    "        elif beta <= max_value:\n",
    "            beta *= 2\n",
    "            rho = 1  # Reset rho to 1 when beta increases\n",
    "\n",
    "        if rho > max_value and beta > max_value:\n",
    "            break\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "# Initial parameters\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "\n",
    "\n",
    "best_rho, best_beta, best_error_rate = search_optimal_parameters(X, y, seed_value, mu, sigma)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best error rate: {best_error_rate}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
