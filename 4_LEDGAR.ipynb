{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels (Y): [97 39 26 45 11 65 73 50 98 47  6  7 43 58 19 47 93 38 65 79 96 85 66 26\n",
      " 13 92 89 58 93 20  7 38 38 26 58  2 97 47 79 95 71 46 76 99 85 79 62  7\n",
      " 84 47 47  7 26 75 87 54 39 26 28 55 47 17 41  2 26 44 79 90 47 74 39 84\n",
      " 65 81  2 73 46 51 41  6 31 26 23 35 33 79 96 65 92 45  7 56 20 85 47 19\n",
      " 42  4 50 68 26 23 15 67 22 28 83 53 16 88 38 25 32 35 65 66 59 26 38 92\n",
      " 38 26 40  7 38 32  2 85 51 38 96 31 95 91 66 99 20 41 66 47 65 32 53 49\n",
      " 26 19 59 18 65 45 16 97 31 93 41 12 79 38 15 38 24 88 79 91 51 88 85 42\n",
      "  6 88 27 69 38 22 65 97 32 74 38 68 79  9 38 88 59 24 41 49 68 20 22 89\n",
      " 83 73 85 75 10 17 57 47 66 38  1 28 26 41 10  5 19 88 77 42 26 22 17 40\n",
      " 51 41 65 29 33 65 99 97 93  6 89 77 11 41 29 37 65 92 49 84 92 76 48 50\n",
      " 61 19 87 58 81  7 18 58 10  6 38 31 83 79 55 82 53 26 29 46 65  7 26 20\n",
      " 38 89 58 81 54 56 89 45 20 26 26 38 16 80 65 35 94 13 47 84 25 10 33 38\n",
      " 97 54  2 13 55 65 65 65 74 26 43 10 31 12 42 47 58 10 95 61 18 24 19  7\n",
      " 19 85  2 20 65 32 80 18 26 31  2 43 24 31 42 33 20 19 26 13 31  4 26 10\n",
      " 16 85  2 29 16 38 26 51 71  9 95  0 87 19 65 79 68 58  7 65 26 23 83 93\n",
      " 85 38 34 15 53  2  6 26 39 51 10 79 10 49 46 55 55 97 16 38 85 53 13 42\n",
      " 38 97 49 65 43 85 10 63 97 19 65 47 83 81 60 87 22 39 92 47 15 63 63 28\n",
      "  7 38 41 97 30 96 38 10 50 38 47 51 65 67 87 36 28 26 89 47  4 54 18 24\n",
      " 74 32 20 77 93 83 87 82 22 46  2 68 38 65 65 43 29 56 47 18 97 47 56  7\n",
      " 99 83 81 71 86 65 87 99 74 26 53 57 45 74 85 47 20 79 65 17 79 26 51 92\n",
      " 59 98 79 38 10 24 90 68 43 79 75 86 42 41 52 38 74  7 47 87]\n",
      "Features Matrix (X): (500, 19975)\n",
      "[ 0  1  2  4  5  6  7  9 10 11 12 13 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51\n",
      " 52 53 54 55 56 57 58 59 60 61 62 63 65 66 67 68 69 71 73 74 75 76 77 79\n",
      " 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def load_and_process_data_to_sparse_matrix(filename):\n",
    "    # Initialize lists for class labels and feature entries\n",
    "    labels = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data_values = []\n",
    "    \n",
    "    # Open and read the file line by line\n",
    "    with open(filename, 'r') as file:\n",
    "        row_index = 0\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            labels.append(int(parts[0]))  # The first part is the class label\n",
    "            \n",
    "            for feature in parts[1:]:\n",
    "                index, value = feature.split(':')\n",
    "                col_indices.append(int(index))\n",
    "                data_values.append(float(value))\n",
    "                row_indices.append(row_index)\n",
    "            \n",
    "            row_index += 1\n",
    "    \n",
    "    # Determine the shape of the matrix\n",
    "    num_rows = row_index\n",
    "    num_cols = max(col_indices) + 1  # Adding 1 because indices start at 0\n",
    "    \n",
    "    # Create the CSR matrix from the row, column, and data value lists\n",
    "    X_sparse = csr_matrix((data_values, (row_indices, col_indices)), shape=(num_rows, num_cols))\n",
    "    \n",
    "    # Convert labels list to a numpy array\n",
    "    Y = np.array(labels)\n",
    "    \n",
    "    return Y, X_sparse\n",
    "\n",
    "# Load and process the data from the 'ist' file into sparse format\n",
    "Y, X_sparse = load_and_process_data_to_sparse_matrix('ledgar_lexglue_tfidft_trainx')\n",
    "print(\"Class Labels (Y):\", Y)\n",
    "print(\"Features Matrix (X):\", X_sparse.shape)\n",
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_5_percent_samples(X, y, seed):\n",
    "    \n",
    "    np.random.seed(seed)  ## Seed has been fixed to 42 in the beginning of the code.\n",
    "\n",
    "    # Determine the number of samples to select from each class (5%)\n",
    "    num_samples_per_class = {label: int(np.ceil(0.05 * np.sum(y == label))) for label in np.unique(y)}\n",
    "\n",
    "    ## For every unique class, I choose the (ceiling_%5) amount of instances. \n",
    "    ## I tried using floor function but the instances per class came out to be too low to get meaningful results.\n",
    "\n",
    "    # Initialize arrays to store the indices of labeled and unlabeled data\n",
    "    labeled_indices = np.array([], dtype=int)\n",
    "    unlabeled_indices = np.array([], dtype=int)\n",
    "\n",
    "    # Select 5% from each class as labeled data\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)[0] \n",
    "        \n",
    "## This will give me the indices of the instances which belong to the class 'label', which is a number from 0 to 2.\n",
    "        np.random.shuffle(indices)\n",
    "        label_indices = indices[:num_samples_per_class[label]]\n",
    "        unlabeled_indices = np.concatenate((unlabeled_indices, indices[num_samples_per_class[label]:]))\n",
    "        labeled_indices = np.concatenate((labeled_indices, label_indices))\n",
    "\n",
    "    # Split the data into labeled and unlabeled sets\n",
    "    X_labeled = X[labeled_indices]\n",
    "    y_labeled = y[labeled_indices]\n",
    "    \n",
    "    X_unlabeled = X[unlabeled_indices]\n",
    "    y_unlabeled = y[unlabeled_indices]\n",
    "    \n",
    "    return X_labeled, y_labeled, X_unlabeled, y_unlabeled\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  ## Damping factor\n",
    "teta = 0.01  ## Treshold value to determine the strong affinities.\n",
    "## These two paramerer values are also mentioned in the paper, p.7 \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "k = 6 ## As sugested in the paper, page 7\n",
    "\n",
    "def construct_neighborhood_indicator_matrix(X):\n",
    "\n",
    "    # Initialize the nearest neighbors model and fit it to the labeled data\n",
    "    nn = NearestNeighbors(algorithm='auto', metric=\"euclidean\").fit(X)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X) ## For this matrix, we are not interested in distances, only indices.\n",
    "\n",
    "    # Initialize the neighborhood indicator matrix P with zeros\n",
    "    P = np.zeros((len(X), len(X)))\n",
    "\n",
    "    # Fill in the neighborhood indicator matrix P for the labeled samples\n",
    "    for i in range(len(X)):\n",
    "        P[i, indices[i]] = 1 / k\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "def affinity_propagation(P, y_labeled, alpha, teta):\n",
    "    # Initialize the affinity matrix W_0\n",
    "    n_samples = P.shape[0]\n",
    "    W_0 = np.zeros((n_samples, n_samples)) ## np.eye(n_samples) ## da olabilir. Tekrar iyi bak.\n",
    "    \n",
    "    ## Initially creates a matix full of zeros with the respective dimensions\n",
    "    \n",
    "    labeled_indices = np.where(y_labeled != -1)[0]\n",
    "\n",
    "    # Set affinities for similar and dissimilar pairs\n",
    "    for i, idx_i in enumerate(labeled_indices):\n",
    "        for j, idx_j in enumerate(labeled_indices):\n",
    "            if y_labeled[i] == y_labeled[j]: ## Initially we mark similar points as if in \n",
    "                W_0[idx_i, idx_j] = 1  ## Similar pairs, as written in the paper\n",
    "            else:\n",
    "                W_0[idx_i, idx_j] = -1  ## Dissimilar pairs\n",
    "    \n",
    "    ## W0 has been created using %5 labeled indices. \n",
    "    ## Now we need to propagate the affinities through the neighborhood structure.\n",
    "\n",
    "    # Propagate affinities through the neighborhood structure\n",
    "    W = np.zeros_like(W_0) ##Creates an empty matrix with the same dimensions as W_0.\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        W = (1 - alpha) * W_0 + alpha * np.dot(P, W)\n",
    "        \n",
    "        # Apply the threshold to determine strong affinities\n",
    "        W[np.abs(W) < teta] = 0\n",
    "    \n",
    "    return W\n",
    "\n",
    "def step_2(W,X):\n",
    "\n",
    "    W1 = np.sum(W, axis=1)  # This computes the sum of each row (axis=1 for rows)\n",
    "    D = np.diag(W1)  # This creates a diagonal matrix D from the vector W1\n",
    "\n",
    "    # Compute the graph Laplacian L\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the matrix T\n",
    "    T = X.T @ L @ X  ##  @ stands for matrix multiplication in numpy\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [[0.16666667 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16666667 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.16666667 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.16666667 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.16666667 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.16666667]]\n",
      "W:  [[ 0.54325285 -0.54765624 -0.54765624 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.70968956  0.41861602 -0.70968956 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.67470073 -0.67470073  0.43902451 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.06816256 -0.06816256 -0.06816256 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.18513574 -0.18513574 -0.18513574 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.06193811 -0.06086964 -0.06193811 ...  0.          0.\n",
      "   0.        ]]\n",
      "T:  [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -1.12303225  0.01443101 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.01964147 -1.35018628 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.00383161  0.00347276 ...  0.          0.\n",
      "  -0.54853569]]\n",
      "P shape:  (500, 500)\n",
      "W shape:  (500, 500)\n",
      "T shape:  (19975, 19975)\n"
     ]
    }
   ],
   "source": [
    "# Use the function to perform affinity propagation\n",
    "alpha = 0.5  # Example value for the damping factor\n",
    "threshold = 0.01  # Example threshold value for strong affinities\n",
    "X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, Y, seed_value)\n",
    "\n",
    "# Construct neighborhood indicator matrix P for the labeled data\n",
    "P = construct_neighborhood_indicator_matrix(X)\n",
    "\n",
    "print(\"P: \", P)  # Show the constructed matrix P for verification\n",
    "row_sums = np.sum(P, axis=1)\n",
    "\n",
    "W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "T = step_2(W,X) ## It is basically an (n * n) square matrix where n = #features of X\n",
    "\n",
    "## Verify the affinity matrix\n",
    "\n",
    "print(\"W: \",W)  \n",
    "print(\"T: \", T)\n",
    "\n",
    "print(\"P shape: \", P.shape)\n",
    "print(\"W shape: \", W.shape)\n",
    "print(\"T shape: \", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_sigma(M, rho, sigma):\n",
    "    # Apply Nesterov's smoothing technique\n",
    "    ## to smooth the l1 term \n",
    "    U_star = np.minimum(rho, np.maximum(M / sigma, -rho))\n",
    "    \n",
    "    return np.trace(U_star.T @ M) - (sigma / 2) * np.linalg.norm(U_star, 'fro')**2\n",
    "\n",
    "def grad_h_sigma(M, rho, sigma):\n",
    "    return np.minimum(rho, np.maximum(M / sigma, -rho))\n",
    "\n",
    "def f(M, Sigma):\n",
    "    fM = -np.log(np.linalg.det(M)) + np.trace(Sigma @ M)\n",
    "    return fM\n",
    "\n",
    "def grad_f(M,Σ):\n",
    "    grad_fM = -np.linalg.inv(M) + Σ\n",
    "    return grad_fM\n",
    "\n",
    "def Σ(X,y_labeled, alpha, beta):\n",
    "    M0 = np.eye(X.shape[1])  # Initialize M0 as the identity matrix\n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    T = step_2(W)\n",
    "    Σ = np.linalg.inv(M0) + beta * T  \n",
    "    return Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALM(M_init, Y_init, mu, rho, sigma, Σ, max_iter=500):\n",
    "    M = M_init\n",
    "    Y = Y_init\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        # Step 1: Update M\n",
    "        grad_h_Y = grad_h_sigma(Y, rho, sigma)\n",
    "        M_next = M - mu * (grad_h_Y + grad_f(M,Σ))\n",
    "        \n",
    "        # Step 2: Update Y\n",
    "        grad_f_M_next = grad_f(M_next,Σ)\n",
    "        Y_next = Y - mu * (grad_f_M_next + grad_h_sigma(M_next, rho, sigma))\n",
    "        \n",
    "        if np.linalg.norm(M_next - M, 'fro') < 1e-6:\n",
    "            break\n",
    "         \n",
    "        # Prepare for next iteration\n",
    "        M = M_next\n",
    "        Y = Y_next\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rho = 10000 ## Tuning (smoothness parameter)\n",
    "beta = 2   ## Tuning parameter\n",
    "mu = 0.000001  # Update step size\n",
    "sigma = 0.000001   # Smoothness parameter\n",
    "\n",
    "def run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta):\n",
    "\n",
    "    # Construct neighborhood indicator matrix P for the labeled data\n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    # Apply affinity propagation to obtain the affinity matrix W\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    # Compute matrix T\n",
    "    T = step_2(W,X)\n",
    "\n",
    "    # Set Σ = np.linalg.inv(M0) + beta * T  \n",
    "    Σ = np.linalg.inv(M0) + beta * T\n",
    "\n",
    "    # Run ALM to obtain the sparse metric matrix M\n",
    "    M = ALM( M0, Y0, mu, rho, sigma, Σ)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=1, metric=\"mahalanobis\" , metric_params={'VI': np.linalg.inv(M)})\n",
    "    knn.fit(X_labeled, y_labeled) ## Model training phase with the help of labeled data points.\n",
    "\n",
    "    # Predict labels for unlabeled data\n",
    "    y_pred = knn.predict(X_unlabeled)\n",
    "    # Calculate accuracy score and error rate\n",
    "    accuracy = accuracy_score(y_unlabeled, y_pred)\n",
    "    error_rate = 1 - accuracy\n",
    "    print(\"S3ML Error rate: \", error_rate)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mGeçerli hücrede veya önceki bir hücrede kod yürütülürken Çekirdek kilitlendi. \n",
      "\u001b[1;31mHatanın olası nedenini belirlemek için lütfen hücrelerdeki kodu inceleyin. \n",
      "\u001b[1;31mDaha fazla bilgi için <a href='https://aka.ms/vscodeJupyterKernelCrash'>buraya</a> tıklayın. \n",
      "\u001b[1;31mDaha fazla ayrıntı için Jupyter <a href='command:jupyter.viewOutput'>günlüğüne</a> bakın."
     ]
    }
   ],
   "source": [
    "def experiment_S3ML(experiment_number,  seed_value,mu, rho, sigma, beta, avg_S3ML_error_rate):\n",
    "    for _ in range(experiment_number):\n",
    "        X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, Y, seed_value)\n",
    "        M0 = np.identity(X.shape[1])\n",
    "        Y0 = np.zeros_like(M0)\n",
    "\n",
    "        avg_S3ML_error_rate += run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "        seed_value += 1\n",
    "        \n",
    "    avg_S3ML_error_rate = avg_S3ML_error_rate/experiment_number\n",
    "\n",
    "    print(\"Average error rate of S3ML after \", experiment_number ,\"tests: \", avg_S3ML_error_rate)\n",
    "\n",
    "experiment_S3ML(1,  seed_value, mu, rho, sigma, beta,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###     Grid search (extra):\n",
    "def search_optimal_parameters(X, y, seed_value, mu, sigma, start_value=1, max_value=1000, experiment_number=1):\n",
    "    best_rho = start_value\n",
    "    best_beta = start_value\n",
    "    best_error_rate = float('inf')\n",
    "\n",
    "    rho = start_value\n",
    "    beta = start_value\n",
    "\n",
    "    while rho <= max_value or beta <= max_value:\n",
    "        avg_error_rate = 0\n",
    "        for _ in range(experiment_number):\n",
    "            X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "            M0 = np.identity(X.shape[1])\n",
    "            Y0 = np.zeros_like(M0)\n",
    "\n",
    "            error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "            seed_value += 1\n",
    "            avg_error_rate += error_rate\n",
    "\n",
    "        avg_error_rate /= experiment_number\n",
    "\n",
    "        print(f\"rho: {rho}, beta: {beta}, avg accuracy rate: {1- avg_error_rate}\")\n",
    "\n",
    "        if avg_error_rate < best_error_rate:\n",
    "            best_error_rate = avg_error_rate\n",
    "            best_rho = rho\n",
    "            best_beta = beta\n",
    "\n",
    "        if rho <= max_value:\n",
    "            rho *= 2\n",
    "\n",
    "        elif beta <= max_value:\n",
    "            beta *= 2\n",
    "            rho = 1  # Reset rho to 1 when beta increases\n",
    "\n",
    "        if rho > max_value and beta > max_value:\n",
    "            break\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "# Initial parameters\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "\n",
    "best_rho, best_beta, best_error_rate = search_optimal_parameters(X, Y, seed_value, mu, sigma)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best accuracy rate: {1- best_error_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random search (extra):\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def random_search_s3ml(X, y, folds, num_samples, rho_range, beta_range, mu, sigma, seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed_value)\n",
    "    best_rho = None\n",
    "    best_beta = None\n",
    "    best_error_rate = float('inf')\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        rho = np.random.uniform(*rho_range)\n",
    "        beta = np.random.uniform(*beta_range)\n",
    "        error_rates = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X_train, y_train, seed_value)\n",
    "            M0 = np.identity(X.shape[1])\n",
    "            Y0 = np.zeros_like(M0)\n",
    "\n",
    "            error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "            error_rates.append(error_rate)\n",
    "            seed_value += 1\n",
    "\n",
    "        avg_error_rate = np.mean(error_rates)\n",
    "        if avg_error_rate < best_error_rate:\n",
    "            best_error_rate = avg_error_rate\n",
    "            best_rho = rho\n",
    "            best_beta = beta\n",
    "\n",
    "        print(f\"Sampling: rho={rho:.2f}, beta={beta:.2f}, Avg Error Rate={avg_error_rate:.4f}\")\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "rho_range = (1e-2, 1e4)  \n",
    "beta_range = (1e-2, 1e2)  \n",
    "\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "folds = 10\n",
    "num_samples = 100  # Number of random samples to test\n",
    "\n",
    "best_rho, best_beta, best_error_rate = random_search_s3ml(X, y, folds, num_samples, rho_range, beta_range, mu, sigma, seed_value)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best error rate: {best_error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search based KFold cross-validation (extra):\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def KFoldC_s3ml(X, y, folds, rho_values, beta_values, mu, sigma, seed_value):\n",
    "    kf = KFold(n_splits= folds, shuffle= True, random_state= seed_value)\n",
    "    best_rho ,best_beta ,best_error_rate = None, None, float('inf')\n",
    "\n",
    "    for rho in rho_values:\n",
    "        for beta in beta_values:\n",
    "            error_rates = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X_train, y_train, seed_value)\n",
    "                M0 = np.identity(X.shape[1])\n",
    "                Y0 = np.zeros_like(M0)\n",
    "\n",
    "                error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "                error_rates.append(error_rate)\n",
    "                seed_value += 1\n",
    "\n",
    "            avg_error_rate = np.mean(error_rates)\n",
    "            if avg_error_rate < best_error_rate:\n",
    "                best_error_rate = avg_error_rate\n",
    "                best_rho = rho\n",
    "                best_beta = beta\n",
    "\n",
    "            print(f\"Grid Search: rho={rho:.2f}, beta={beta:.2f}, Avg Error Rate={avg_error_rate:.4f}\")\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "rho_values = np.logspace(-2, 4, num=10)  # Logarithmically spaced values from 0.01 to 10000\n",
    "beta_values = np.logspace(-2, 2, num=10)  # Logarithmically spaced values from 0.01 to 100\n",
    "\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "folds = 10\n",
    "\n",
    "best_rho, best_beta, best_error_rate = KFoldC_s3ml(X, y, folds, rho_values, beta_values, mu, sigma, seed_value)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best error rate: {best_error_rate:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
