{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels (Y): [  2   3   5   6   8   9  11  12  14  15  17  18  20  21  23  24  26  27\n",
      "  29  30  32  33  35  36  38  39  41  42  44  45  47  48  50  51  53  54\n",
      "  56  57  59  60  62  63  65  66  68  69  71  72  74  75  77  78  80  81\n",
      "  83  84  86  87  89  90  92  93  95  96  98  99 101 102 104 105   2   3\n",
      "   5   6   8   9  11  12  14  15  17  18  20  21  23  24  26  27  29  30\n",
      "  32  33  35  36  38  39  41  42  44  45  47  48  50  51  53  54  56  57\n",
      "  59  60  62  63  65  66  68  69  71  72  74  75  77  78  80  81  83  84\n",
      "  86  87  89  90  92  93  95  96  98  99 101 102 104 105   2   3   5   6\n",
      "   8   9  11  12  14  15  17  18  20  21  23  24  26  27  29  30  32  33\n",
      "  35  36  38  39  41  42  44  45  47  48  50  51  53  54  56  57  59  60\n",
      "  62  63  65  66  68  69  71  72  74  75  77  78  80  81  83  84  86  87\n",
      "  89  90  92  93  95  96  98  99 101 102 104 105   2   3   5   6   8   9\n",
      "  11  12  14  15  17  18  20  21  23  24  26  27  29  30  32  33  35  36\n",
      "  38  39  41  42  44  45  47  48  50  51  53  54  56  57  59  60  62  63\n",
      "  65  66  68  69  71  72  74  75  77  78  80  81  83  84  86  87  89  90\n",
      "  92  93  95  96  98  99 101 102 104 105   2   3   5   6   8   9  11  12\n",
      "  14  15  17  18  20  21  23  24  26  27  29  30  32  33  35  36  38  39\n",
      "  41  42  44  45  47  48  50  51  53  54  56  57  59  60  62  63  65  66\n",
      "  68  69  71  72  74  75  77  78  80  81  83  84  86  87  89  90  92  93\n",
      "  95  96  98  99 101 102 104 105   2   3   5   6   8   9  11  12  14  15\n",
      "  17  18  20  21  23  24  26  27  29  30  32  33  35  36  38  39  41  42\n",
      "  44  45  47  48  50  51  53  54  56  57  59  60  62  63  65  66  68  69\n",
      "  71  72  74  75  77  78  80  81  83  84  86  87  89  90  92  93  95  96\n",
      "  98  99 101 102 104 105]\n",
      "SHAPE (Y): (420,)\n",
      "Features Matrix (X): (420, 55185)\n",
      "Unique classes:  [  2   3   5   6   8   9  11  12  14  15  17  18  20  21  23  24  26  27\n",
      "  29  30  32  33  35  36  38  39  41  42  44  45  47  48  50  51  53  54\n",
      "  56  57  59  60  62  63  65  66  68  69  71  72  74  75  77  78  80  81\n",
      "  83  84  86  87  89  90  92  93  95  96  98  99 101 102 104 105]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "seed_value = 42   ## An arbitrary seed value\n",
    "\n",
    "def load_and_process_data(filename):\n",
    "    # Initialize lists for class labels and feature dictionaries\n",
    "    labels = []\n",
    "    feature_dicts = []\n",
    "    \n",
    "    # Open and read the file line by line\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            labels.append(int(parts[0]))  # The first part is the class label\n",
    "            \n",
    "            # Create a dictionary for the features in this row\n",
    "            features = {}\n",
    "            for feature in parts[1:]:\n",
    "                index, value = feature.split(':')\n",
    "                features[int(index)] = float(value)\n",
    "            feature_dicts.append(features)\n",
    "    \n",
    "    # Determine the maximum feature index to size the feature matrix\n",
    "    max_index = max(max(d.keys()) for d in feature_dicts if d)\n",
    "    \n",
    "    # Create the feature matrix initialized to zero\n",
    "    X = np.zeros((len(feature_dicts), max_index + 1))  # +1 because index starts from 0\n",
    "    \n",
    "    # Populate the feature matrix using the dictionaries\n",
    "    for i, features in enumerate(feature_dicts):\n",
    "        for index, value in features.items():\n",
    "            X[i, index] = value\n",
    "    \n",
    "    # Convert labels list to a numpy array\n",
    "    Y = np.array(labels)\n",
    "    \n",
    "    return Y, X\n",
    "\n",
    "Y, X = load_and_process_data('sector.scalex')\n",
    "\n",
    "print(\"Class Labels (Y):\", Y)\n",
    "print(\"SHAPE (Y):\", Y.shape)\n",
    "print(\"Features Matrix (X):\", X.shape)\n",
    "print('Unique classes: ',np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_5_percent_samples(X, y, seed):\n",
    "\n",
    "    np.random.seed(seed)  ## Seed has been fixed to 42 in the beginning of the code.\n",
    "\n",
    "    # Determine the number of samples to select from each class (5%)\n",
    "    num_samples_per_class = {label: int(np.ceil(0.05 * np.sum(y == label))) for label in np.unique(y)}\n",
    "\n",
    "    ## For every unique class, I choose the (ceiling_%5) amount of instances. \n",
    "    ## I tried using floor function but the instances per class came out to be too low to get meaningful results.\n",
    "\n",
    "    # Initialize arrays to store the indices of labeled and unlabeled data\n",
    "    labeled_indices = np.array([], dtype=int)\n",
    "    unlabeled_indices = np.array([], dtype=int)\n",
    "\n",
    "    # Select 5% from each class as labeled data\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)[0] \n",
    "        \n",
    "## This will give me the indices of the instances which belong to the class 'label', which is a number from 0 to 2.\n",
    "        np.random.shuffle(indices)\n",
    "        label_indices = indices[:num_samples_per_class[label]]\n",
    "        unlabeled_indices = np.concatenate((unlabeled_indices, indices[num_samples_per_class[label]:]))\n",
    "        labeled_indices = np.concatenate((labeled_indices, label_indices))\n",
    "\n",
    "    # Split the data into labeled and unlabeled sets\n",
    "    X_labeled = X[labeled_indices]\n",
    "    y_labeled = y[labeled_indices]\n",
    "    \n",
    "    X_unlabeled = X[unlabeled_indices]\n",
    "    y_unlabeled = y[unlabeled_indices]\n",
    "    \n",
    "    return X_labeled, y_labeled, X_unlabeled, y_unlabeled\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  ## Damping factor\n",
    "teta = 0.01  ## Treshold value to determine the strong affinities.\n",
    "## These two paramerer values are also mentioned in the paper, p.7 \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "k = 6 ## As sugested in the paper, page 7\n",
    "\n",
    "def construct_neighborhood_indicator_matrix(X):\n",
    "\n",
    "    # Initialize the nearest neighbors model and fit it to the labeled data\n",
    "    nn = NearestNeighbors(algorithm='auto', metric=\"euclidean\").fit(X)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X) ## For this matrix, we are not interested in distances, only indices.\n",
    "\n",
    "    # Initialize the neighborhood indicator matrix P with zeros\n",
    "    P = np.zeros((len(X), len(X)))\n",
    "\n",
    "    # Fill in the neighborhood indicator matrix P for the labeled samples\n",
    "    for i in range(len(X)):\n",
    "        P[i, indices[i]] = 1 / k\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "def affinity_propagation(P, y_labeled, alpha, teta):\n",
    "    # Initialize the affinity matrix W_0\n",
    "    n_samples = P.shape[0]\n",
    "    W_0 = np.zeros((n_samples, n_samples)) ## np.eye(n_samples) ## da olabilir. Tekrar iyi bak.\n",
    "    \n",
    "    ## Initially creates a matix full of zeros with the respective dimensions\n",
    "    \n",
    "    labeled_indices = np.where(y_labeled != -1)[0]\n",
    "\n",
    "    # Set affinities for similar and dissimilar pairs\n",
    "    for i, idx_i in enumerate(labeled_indices):\n",
    "        for j, idx_j in enumerate(labeled_indices):\n",
    "            if y_labeled[i] == y_labeled[j]: ## Initially we mark similar points as if in \n",
    "                W_0[idx_i, idx_j] = 1  ## Similar pairs, as written in the paper\n",
    "            else:\n",
    "                W_0[idx_i, idx_j] = -1  ## Dissimilar pairs\n",
    "    \n",
    "    ## W0 has been created using %5 labeled indices. \n",
    "    ## Now we need to propagate the affinities through the neighborhood structure.\n",
    "\n",
    "    # Propagate affinities through the neighborhood structure\n",
    "    W = np.zeros_like(W_0) ##Creates an empty matrix with the same dimensions as W_0.\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        W = (1 - alpha) * W_0 + alpha * np.dot(P, W)\n",
    "        \n",
    "        # Apply the threshold to determine strong affinities\n",
    "        W[np.abs(W) < teta] = 0\n",
    "    \n",
    "    return W\n",
    "\n",
    "def step_2(W,X):\n",
    "\n",
    "    W1 = np.sum(W, axis=1)  # This computes the sum of each row (axis=1 for rows)\n",
    "    D = np.diag(W1)  # This creates a diagonal matrix D from the vector W1\n",
    "\n",
    "    # Compute the graph Laplacian L\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the matrix T\n",
    "    T = X.T @ L @ X  ##  @ stands for matrix multiplication in numpy\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  [[0.16666667 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16666667 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.16666667 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.16666667 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.16666667 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Use the function to perform affinity propagation\n",
    "alpha = 0.5  # Example value for the damping factor\n",
    "threshold = 0.01  # Example threshold value for strong affinities\n",
    "X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, Y, seed_value)\n",
    "\n",
    "# Construct neighborhood indicator matrix P for the labeled data\n",
    "P = construct_neighborhood_indicator_matrix(X)\n",
    "\n",
    "print(\"P: \", P)  # Show the constructed matrix P for verification\n",
    "row_sums = np.sum(P, axis=1)\n",
    "\n",
    "W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "T = step_2(W,X) ## It is basically an (n * n) square matrix where n = #features of X\n",
    "\n",
    "## Verify the affinity matrix\n",
    "\n",
    "print(\"W: \",W)  \n",
    "print(\"T: \", T)\n",
    "\n",
    "print(\"P shape: \", P.shape)\n",
    "print(\"W shape: \", W.shape)\n",
    "print(\"T shape: \", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_sigma(M, rho, sigma):\n",
    "    # Apply Nesterov's smoothing technique\n",
    "    ## to smooth the l1 term \n",
    "    U_star = np.minimum(rho, np.maximum(M / sigma, -rho))\n",
    "    \n",
    "    return np.trace(U_star.T @ M) - (sigma / 2) * np.linalg.norm(U_star, 'fro')**2\n",
    "\n",
    "def grad_h_sigma(M, rho, sigma):\n",
    "    return np.minimum(rho, np.maximum(M / sigma, -rho))\n",
    "\n",
    "def f(M, Sigma):\n",
    "    fM = -np.log(np.linalg.det(M)) + np.trace(Sigma @ M)\n",
    "    return fM\n",
    "\n",
    "def grad_f(M,Σ):\n",
    "    grad_fM = -np.linalg.inv(M) + Σ\n",
    "    return grad_fM\n",
    "\n",
    "def Σ(X,y_labeled, alpha, beta):\n",
    "    M0 = np.eye(X.shape[1])  \n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    T = step_2(W)\n",
    "    Σ = np.linalg.inv(M0) + beta * T  \n",
    "    return Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALM(M_init, Y_init, mu, rho, sigma, Σ, max_iter=500):\n",
    "    M = M_init\n",
    "    Y = Y_init\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        grad_h_Y = grad_h_sigma(Y, rho, sigma)\n",
    "        M_next = M - mu * (grad_h_Y + grad_f(M,Σ))\n",
    "        \n",
    "        grad_f_M_next = grad_f(M_next,Σ)\n",
    "        Y_next = Y - mu * (grad_f_M_next + grad_h_sigma(M_next, rho, sigma))\n",
    "        \n",
    "        if np.linalg.norm(M_next - M, 'fro') < 1e-6:\n",
    "            break\n",
    "         \n",
    "        M = M_next\n",
    "        Y = Y_next\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rho = 10000 ## Tuning (smoothness parameter)\n",
    "beta = 2   ## Tuning parameter\n",
    "mu = 0.000001  # Update step size\n",
    "sigma = 0.000001   # Smoothness parameter\n",
    "\n",
    "def run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta):\n",
    "\n",
    "    # Construct neighborhood indicator matrix P for the labeled data\n",
    "    P = construct_neighborhood_indicator_matrix(X)\n",
    "    # Apply affinity propagation to obtain the affinity matrix W\n",
    "    W = affinity_propagation(P, y_labeled, alpha, teta)\n",
    "    # Compute matrix T\n",
    "    T = step_2(W,X)\n",
    "\n",
    "    # Set Σ = np.linalg.inv(M0) + beta * T  \n",
    "    Σ = np.linalg.inv(M0) + beta * T\n",
    "\n",
    "    # Run ALM to obtain the sparse metric matrix M\n",
    "    M = ALM( M0, Y0, mu, rho, sigma, Σ)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=1, metric=\"mahalanobis\" , metric_params={'VI': np.linalg.inv(M)})\n",
    "    knn.fit(X_labeled, y_labeled) ## Model training phase with the help of labeled data points.\n",
    "\n",
    "    # Predict labels for unlabeled data\n",
    "    y_pred = knn.predict(X_unlabeled)\n",
    "    # Calculate accuracy score and error rate\n",
    "    accuracy = accuracy_score(y_unlabeled, y_pred)\n",
    "    error_rate = 1 - accuracy\n",
    "    print(\"S3ML Error rate: \", error_rate)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_S3ML(experiment_number,  seed_value,mu, rho, sigma, beta, avg_S3ML_error_rate):\n",
    "    for _ in range(experiment_number):\n",
    "        X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, Y, seed_value)\n",
    "        M0 = np.identity(X.shape[1])\n",
    "        Y0 = np.zeros_like(M0)\n",
    "\n",
    "        avg_S3ML_error_rate += run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "        seed_value += 1\n",
    "        \n",
    "    avg_S3ML_error_rate = avg_S3ML_error_rate/experiment_number\n",
    "\n",
    "    print(\"Average error rate of S3ML after \", experiment_number ,\"tests: \", avg_S3ML_error_rate)\n",
    "\n",
    "experiment_S3ML(1,  seed_value, mu, rho, sigma, beta,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###     Grid search (extra):\n",
    "def search_optimal_parameters(X, y, seed_value, mu, sigma, start_value=1, max_value=1000, experiment_number=1):\n",
    "    best_rho = start_value\n",
    "    best_beta = start_value\n",
    "    best_error_rate = float('inf')\n",
    "\n",
    "    rho = start_value\n",
    "    beta = start_value\n",
    "\n",
    "    while rho <= max_value or beta <= max_value:\n",
    "        avg_error_rate = 0\n",
    "        for _ in range(experiment_number):\n",
    "            X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X, y, seed_value)\n",
    "            M0 = np.identity(X.shape[1])\n",
    "            Y0 = np.zeros_like(M0)\n",
    "\n",
    "            error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "            seed_value += 1\n",
    "            avg_error_rate += error_rate\n",
    "\n",
    "        avg_error_rate /= experiment_number\n",
    "\n",
    "        print(f\"rho: {rho}, beta: {beta}, avg accuracy rate: {1- avg_error_rate}\")\n",
    "\n",
    "        if avg_error_rate < best_error_rate:\n",
    "            best_error_rate = avg_error_rate\n",
    "            best_rho = rho\n",
    "            best_beta = beta\n",
    "\n",
    "        if rho <= max_value:\n",
    "            rho *= 2\n",
    "\n",
    "        elif beta <= max_value:\n",
    "            beta *= 2\n",
    "            rho = 1  # Reset rho to 1 when beta increases\n",
    "\n",
    "        if rho > max_value and beta > max_value:\n",
    "            break\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "# Initial parameters\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "\n",
    "best_rho, best_beta, best_error_rate = search_optimal_parameters(X, Y, seed_value, mu, sigma)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best accuracy rate: {1- best_error_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random search (extra):\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def random_search_s3ml(X, y, folds, num_samples, rho_range, beta_range, mu, sigma, seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed_value)\n",
    "    best_rho = None\n",
    "    best_beta = None\n",
    "    best_error_rate = float('inf')\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        rho = np.random.uniform(*rho_range)\n",
    "        beta = np.random.uniform(*beta_range)\n",
    "        error_rates = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X_train, y_train, seed_value)\n",
    "            M0 = np.identity(X.shape[1])\n",
    "            Y0 = np.zeros_like(M0)\n",
    "\n",
    "            error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "            error_rates.append(error_rate)\n",
    "            seed_value += 1\n",
    "\n",
    "        avg_error_rate = np.mean(error_rates)\n",
    "        if avg_error_rate < best_error_rate:\n",
    "            best_error_rate = avg_error_rate\n",
    "            best_rho = rho\n",
    "            best_beta = beta\n",
    "\n",
    "        print(f\"Sampling: rho={rho:.2f}, beta={beta:.2f}, Avg Error Rate={avg_error_rate:.4f}\")\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "rho_range = (1e-2, 1e4)  \n",
    "beta_range = (1e-2, 1e2)  \n",
    "\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "folds = 10\n",
    "num_samples = 100  # Number of random samples to test\n",
    "\n",
    "best_rho, best_beta, best_error_rate = random_search_s3ml(X, y, folds, num_samples, rho_range, beta_range, mu, sigma, seed_value)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best error rate: {best_error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search based KFold cross-validation (extra):\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def KFoldC_s3ml(X, y, folds, rho_values, beta_values, mu, sigma, seed_value):\n",
    "    kf = KFold(n_splits= folds, shuffle= True, random_state= seed_value)\n",
    "    best_rho ,best_beta ,best_error_rate = None, None, float('inf')\n",
    "\n",
    "    for rho in rho_values:\n",
    "        for beta in beta_values:\n",
    "            error_rates = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                X_labeled, y_labeled, X_unlabeled, y_unlabeled = select_5_percent_samples(X_train, y_train, seed_value)\n",
    "                M0 = np.identity(X.shape[1])\n",
    "                Y0 = np.zeros_like(M0)\n",
    "\n",
    "                error_rate = run_s3ml(X_labeled, y_labeled, X_unlabeled, y_unlabeled, M0, Y0, mu, rho, sigma, beta)\n",
    "                error_rates.append(error_rate)\n",
    "                seed_value += 1\n",
    "\n",
    "            avg_error_rate = np.mean(error_rates)\n",
    "            if avg_error_rate < best_error_rate:\n",
    "                best_error_rate = avg_error_rate\n",
    "                best_rho = rho\n",
    "                best_beta = beta\n",
    "\n",
    "            print(f\"Grid Search: rho={rho:.2f}, beta={beta:.2f}, Avg Error Rate={avg_error_rate:.4f}\")\n",
    "\n",
    "    return best_rho, best_beta, best_error_rate\n",
    "\n",
    "rho_values = np.logspace(-2, 4, num=10)  # Logarithmically spaced values from 0.01 to 10000\n",
    "beta_values = np.logspace(-2, 2, num=10)  # Logarithmically spaced values from 0.01 to 100\n",
    "\n",
    "mu = 1e-6\n",
    "sigma = 1e-6\n",
    "seed_value = 42\n",
    "folds = 10\n",
    "\n",
    "best_rho, best_beta, best_error_rate = KFoldC_s3ml(X, y, folds, rho_values, beta_values, mu, sigma, seed_value)\n",
    "print(f\"Best rho: {best_rho}, Best beta: {best_beta}, Best error rate: {best_error_rate:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
